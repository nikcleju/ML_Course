<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>ML Laboratory 01: Linear Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="L1_LinearRegression_files/libs/clipboard/clipboard.min.js"></script>
<script src="L1_LinearRegression_files/libs/quarto-html/quarto.js"></script>
<script src="L1_LinearRegression_files/libs/quarto-html/popper.min.js"></script>
<script src="L1_LinearRegression_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="L1_LinearRegression_files/libs/quarto-html/anchor.min.js"></script>
<link href="L1_LinearRegression_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="L1_LinearRegression_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="L1_LinearRegression_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="L1_LinearRegression_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="L1_LinearRegression_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#objective" id="toc-objective" class="nav-link active" data-scroll-target="#objective"><span class="toc-section-number">1</span>  Objective</a></li>
  <li><a href="#theoretical-aspects" id="toc-theoretical-aspects" class="nav-link" data-scroll-target="#theoretical-aspects"><span class="toc-section-number">2</span>  Theoretical aspects</a>
  <ul class="collapse">
  <li><a href="#regression-vs-classification" id="toc-regression-vs-classification" class="nav-link" data-scroll-target="#regression-vs-classification"><span class="toc-section-number">2.1</span>  Regression vs Classification</a></li>
  <li><a href="#linear-regression-the-model" id="toc-linear-regression-the-model" class="nav-link" data-scroll-target="#linear-regression-the-model"><span class="toc-section-number">2.2</span>  Linear regression: the model</a></li>
  <li><a href="#linear-regression-the-parameters" id="toc-linear-regression-the-parameters" class="nav-link" data-scroll-target="#linear-regression-the-parameters"><span class="toc-section-number">2.3</span>  Linear regression: the parameters</a></li>
  <li><a href="#cost-function-loss-function" id="toc-cost-function-loss-function" class="nav-link" data-scroll-target="#cost-function-loss-function"><span class="toc-section-number">2.4</span>  Cost function (loss function)</a></li>
  <li><a href="#matrix-form-of-linear-regression" id="toc-matrix-form-of-linear-regression" class="nav-link" data-scroll-target="#matrix-form-of-linear-regression"><span class="toc-section-number">2.5</span>  Matrix form of linear regression</a></li>
  <li><a href="#how-to-train-linear-regression" id="toc-how-to-train-linear-regression" class="nav-link" data-scroll-target="#how-to-train-linear-regression"><span class="toc-section-number">2.6</span>  How to train linear regression?</a>
  <ul class="collapse">
  <li><a href="#a.-closed-form-solution" id="toc-a.-closed-form-solution" class="nav-link" data-scroll-target="#a.-closed-form-solution">A. Closed form solution</a></li>
  <li><a href="#b.-matlab-function-doing-the-job-for-us" id="toc-b.-matlab-function-doing-the-job-for-us" class="nav-link" data-scroll-target="#b.-matlab-function-doing-the-job-for-us">B. Matlab function doing the job for us</a></li>
  <li><a href="#c.-matlab-gui-app-doing-the-job-for-us" id="toc-c.-matlab-gui-app-doing-the-job-for-us" class="nav-link" data-scroll-target="#c.-matlab-gui-app-doing-the-job-for-us">C. Matlab GUI App doing the job for us</a></li>
  <li><a href="#d.-optimization-with-gradient-descent" id="toc-d.-optimization-with-gradient-descent" class="nav-link" data-scroll-target="#d.-optimization-with-gradient-descent">D. Optimization with Gradient Descent</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#practical-work" id="toc-practical-work" class="nav-link" data-scroll-target="#practical-work"><span class="toc-section-number">3</span>  Practical work</a></li>
  <li><a href="#final-questions" id="toc-final-questions" class="nav-link" data-scroll-target="#final-questions"><span class="toc-section-number">4</span>  Final questions</a></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">ML Laboratory 01: Linear Regression</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="objective" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Objective</h1>
<p>Students should understand and be able use a linear regression model in Matlab</p>
</section>
<section id="theoretical-aspects" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Theoretical aspects</h1>
<section id="regression-vs-classification" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="regression-vs-classification"><span class="header-section-number">2.1</span> Regression vs Classification</h2>
<p>A typical job in Machine Learning is to <strong>predict</strong> an output <span class="math inline">\(y\)</span> from some given data <span class="math inline">\((x_1, ... x_n)\)</span>:</p>
<p><span class="math display">\[X = \begin{bmatrix} x_1 &amp; x_2 &amp; \dots &amp; x_N \end{bmatrix} \rightarrow y\]</span></p>
<p>In <strong>supervised learning</strong>, we are given many examples (input-output pairs) out of which we need to deduce the prediction rule.</p>
<p>Depending on the meaning of <span class="math inline">\(y\)</span>, we can have:</p>
<ul>
<li><strong>classification</strong>: <span class="math inline">\(y\)</span> is a number representing a category (0 = cat, 1 = dog, 2 = bird). The numbers have no real meaning as numbers, whey are just numerical labels for representing the categories.</li>
<li><strong>regression</strong>: <span class="math inline">\(y\)</span> is a number which actually means a numerical result (it is the price of a house, or a probability, etc.). Regression is also known as “curve fitting”.</li>
</ul>
</section>
<section id="linear-regression-the-model" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="linear-regression-the-model"><span class="header-section-number">2.2</span> Linear regression: the model</h2>
<p>The linear regression model: the output is assumed to be a <strong>linear combination</strong> of the inputs.</p>
<p><span class="math display">\[y \approx w_1 x_1 + w_2 x_2 + ... + w_N x_N + b.\]</span></p>
<p>The coefficients <span class="math inline">\(w_i\)</span> and <span class="math inline">\(b\)</span> are parameters we need to estimate/find. “Learning” means finding good values for the parameters, which get the job done.</p>
</section>
<section id="linear-regression-the-parameters" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="linear-regression-the-parameters"><span class="header-section-number">2.3</span> Linear regression: the parameters</h2>
<p>The parameters of the linear regression model are the <strong>weights</strong> <span class="math inline">\(w_1, w_2, ... w_N\)</span> and the <strong>bias</strong> value <span class="math inline">\(b\)</span> (also known as the <strong>intercept</strong>).</p>
</section>
<section id="cost-function-loss-function" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="cost-function-loss-function"><span class="header-section-number">2.4</span> Cost function (loss function)</h2>
<p>Given some parameters <span class="math inline">\(w_i\)</span> and <span class="math inline">\(b\)</span>, how do we know if they are good?</p>
<p>For an input vector <span class="math inline">\(X\)</span>:</p>
<ul>
<li>we compute the <strong>prediction</strong>: <span class="math display">\[\hat{y} = w_1 x_1 + w_2 x_2 + ... + w_N x_N + b\]</span></li>
<li>we compare the prediction against the true result (“<strong>ground-truth</strong>”) with the <strong>cost function</strong> (also known as <strong>loss function</strong>): <span class="math display">\[J = (\hat{y} - y)^2\]</span></li>
</ul>
<p>The cost function defines what is good and what is bad, depending on its result (the cost):</p>
<ul>
<li>cost is small =&gt; prediction is good</li>
<li>cost is big =&gt; prediction is bad</li>
</ul>
<p>If we have many data (input-output pairs), the overall cost is the average of the cost for each entry: <span class="math display">\[J = \frac{1}{N} \sum_i (\hat{y}^i - y^i)^2\]</span></p>
<p>The cost function can be anything. Here, and typically for linear regression, we have the <strong>quadratic cost function</strong> (also known as “least squares”, “<span class="math inline">\(\ell_2\)</span> norm”, …). This is typical good cost function for regression, but not so good for classification problems. <span class="math display">\[(\hat{y} - y)^2\]</span></p>
<p>Other cost functions can be used, and they lead to different results (sometimes better, sometimes worse, depending on the problem).</p>
</section>
<section id="matrix-form-of-linear-regression" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="matrix-form-of-linear-regression"><span class="header-section-number">2.5</span> Matrix form of linear regression</h2>
<p>The linear regression problem can be written in matrix form as follows:</p>
<p><span class="math display">\[\begin{bmatrix} y^1 \\ y^2 \\ \vdots \\ y^N\end{bmatrix}
\approx
\begin{bmatrix} \hat{y}^1 \\ \hat{y}^2 \\ \vdots \\\hat{y}^N\end{bmatrix}
=
\begin{bmatrix}
x_1^1 &amp; x_2^1 &amp; x_3^1 &amp; ... &amp; x_{11}^1 &amp; 1 \\
x_1^2 &amp; x_2^2 &amp; x_3^2 &amp; ... &amp; x_{11}^2 &amp; 1 \\
\vdots &amp; \vdots &amp; \vdots &amp; ... &amp; \vdots &amp; \vdots \\
x_1^N &amp; x_2^N &amp; x_3^N &amp; ... &amp; x_{11}^N &amp; 1 \\
\end{bmatrix}
\cdot
\begin{bmatrix} w_1 \\ w_2 \\ \vdots \\ w_N \\ b \end{bmatrix}\]</span></p>
<p>Naming the matrices and vectors as <span class="math inline">\(Y\)</span>, <span class="math inline">\(\hat{Y}\)</span>, <span class="math inline">\(X\)</span>, <span class="math inline">\(W\)</span>, we have: <span class="math display">\[Y \approx \hat{Y} = X \cdot W\]</span></p>
<p>Note two important things:</p>
<ul>
<li>We can treat <span class="math inline">\(b\)</span> just like another weight, which multiplies some constant value 1. We can extend the input matrix with a column of 1’s, and consider <span class="math inline">\(b\)</span> just like the 12-th weight in <span class="math inline">\(W\)</span>.</li>
<li>The same weights appear in all linear combinations. They are the unknowns in this linear equation system.</li>
<li>This is a <strong>massively overcomplete</strong> equation system. There is probably no exact solution, but there exists a <strong>least-squares solution</strong>, i.e.&nbsp;the solution vector <span class="math inline">\(W\)</span> which makes the predictions <span class="math inline">\(\hat{Y}\)</span> <strong>as close as possible</strong> to the true values <span class="math inline">\(Y\)</span> (i.e.&nbsp;minimum cost).</li>
</ul>
</section>
<section id="how-to-train-linear-regression" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="how-to-train-linear-regression"><span class="header-section-number">2.6</span> How to train linear regression?</h2>
<p><strong>Training</strong> = <strong>learning</strong> = finding good values for the unknown parameters.</p>
<p>For linear regression, we can do it in three ways:</p>
<section id="a.-closed-form-solution" class="level3">
<h3 class="anchored" data-anchor-id="a.-closed-form-solution">A. Closed form solution</h3>
<p>When the cost function is the quadratic, the best solution can actually be found analytically (this may be the only such case in the whole of Machine Learning :) ):</p>
<p><span class="math display">\[W_{optimal} = X^\dagger Y = \left( X^* X \right)^{-1} X^* \cdot Y\]</span></p>
<p>This is not true anymore if we change the cost function.</p>
<p>The sign <code>*</code> signifies <strong>transposition</strong>.</p>
</section>
<section id="b.-matlab-function-doing-the-job-for-us" class="level3">
<h3 class="anchored" data-anchor-id="b.-matlab-function-doing-the-job-for-us">B. Matlab function doing the job for us</h3>
<p>Linear regression can be fitted in Matlab using the function <code>fitml()</code>:</p>
<pre class="{matlab}"><code>mdl = fitlm(X,Y)  % X are the inputs, Y is the target vector, mdl is a model object</code></pre>
</section>
<section id="c.-matlab-gui-app-doing-the-job-for-us" class="level3">
<h3 class="anchored" data-anchor-id="c.-matlab-gui-app-doing-the-job-for-us">C. Matlab GUI App doing the job for us</h3>
<p>Matlab has a nice GUI tool for regression estimation in Apps -&gt; Regression Learner.</p>
<p>Play around with the tool and fit a linear regression model to the data</p>
</section>
<section id="d.-optimization-with-gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="d.-optimization-with-gradient-descent">D. Optimization with Gradient Descent</h3>
<p>The cost function <span class="math inline">\(J\)</span> is a function like any other, and it depends on the parameters <span class="math inline">\(w_i\)</span>.</p>
<p>We can compute the derivative of <span class="math inline">\(J\)</span> with respect to each parameter, <span class="math inline">\(\frac{d J}{d w_i}\)</span>.</p>
<p>The derivative tells us how the cost <span class="math inline">\(J\)</span> changes for a small increase in the parameter <span class="math inline">\(w_i\)</span>. We want to reduce the cost function. If the derivative is positive, we’ll make <span class="math inline">\(w_i\)</span> a little smaller. If it is negative, we’ll increase it a bit.</p>
<p><strong>Gradient Descent optimization procedure</strong>:</p>
<ol type="1">
<li><p>set <span class="math inline">\(\mu\)</span> = step size = small (e.g.&nbsp;0.001)</p></li>
<li><p>initialize parameters <span class="math inline">\(w_i\)</span> somehow (random)</p></li>
<li><p>Repeat:</p>
<ul>
<li>compute predictions and cost J</li>
<li>compute derivative of cost with respect to parameters <span class="math inline">\(\frac{d J}{d w_i}\)</span>.</li>
<li>update each parameter like: <span class="math display">\[ w_i = w_i - \mu \cdot \frac{d J}{d w_i}\]</span></li>
</ul></li>
</ol>
<p>If we group all derivatives in a vector, this is known as the <strong>gradient</strong> vector: <span class="math display">\[\nabla W = \begin{bmatrix} \frac{d J}{d w_1} &amp; \frac{d J}{d w_2} &amp; ... &amp; \frac{d J}{d w_{k}}\end{bmatrix}.\]</span></p>
<p>In matrix form, the update rule can be written as: <span class="math display">\[W = W - \mu \nabla W.\]</span></p>
<p>For linear regression with the quadratic cost function, it can be shown that the gradient is equal to: <span class="math display">\[\nabla W = X^* (\hat{Y} - Y)\]</span></p>
</section>
</section>
</section>
<section id="practical-work" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Practical work</h1>
<p>The data used in this example comes from here: https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009</p>
<p>The data contains 11 numerical chemical measurements for some different brands of red wines, together with a quality score indicated by buyers. The job is to determine how do the measured parameters influence the quality score. The ultimate goal is to predict the quality for some new type of red wine, based on its measurements.</p>
<p>Inputs:</p>
<ul>
<li>1 - fixed acidity</li>
<li>2 - volatile acidity</li>
<li>3 - citric acid</li>
<li>4 - residual sugar</li>
<li>5 - chlorides</li>
<li>6 - free sulfur dioxide</li>
<li>7 - total sulfur dioxide</li>
<li>8 - density</li>
<li>9 - pH</li>
<li>10 - sulphates</li>
<li>11 - alcohol</li>
</ul>
<p>Outputs:</p>
<ul>
<li>12 - quality</li>
</ul>
<p>Let’s load the data first:</p>
<pre class="{matlab}"><code>Data = readmatrix('winequality-red.csv');
X = Data(:,1:11);   % 11 columns for the inputs
Y = Data(:,13);     % 1 column for the output
N = size(Data,1);   % The number of wines in the set (1599)</code></pre>
<p>Extend the X matrix so we can treat the bias <span class="math inline">\(b\)</span> as just another weight.</p>
<pre class="{matlab}"><code>X2 = [X ones(N,1)]</code></pre>
<p>Let’s initialize the weights to some random values</p>
<pre class="{matlab}"><code>W = randn(12, 1)   % a column vector</code></pre>
<p><strong>Task 1</strong>: Compute and show the cost function with the above weights</p>
<pre class="{matlab}"><code>%======================
% Your code here
%======================</code></pre>
<p><strong>Task 2</strong>: Compute the solution with the closed-form formula</p>
<ul>
<li><strong>Question</strong>: According to the model parameters, which is the most important factor in determining the perceived quality of the wine?</li>
</ul>
<pre class="{matlab}"><code>%======================
% Your code here
%======================</code></pre>
<p><strong>Task 3</strong>: Repeat with normalized data</p>
<p>The above question can be misleading, because our data may have different scales. To have a fair comparison, and to help the numerical algorithm, it’s recommended that the data is <strong>normalized</strong>.</p>
<p>Normalization means, in this context, to make each column (each feature) have mean = 0 and standard deviation (or variance) = 1. This is achieved with the following transformation:</p>
<p><span class="math display">\[x \rightarrow \frac{x - \mu}{\sigma},\]</span></p>
<p>where <span class="math inline">\(\mu\)</span> is the mean and <span class="math inline">\(\sigma\)</span> is the standard deviation of every input column (<span class="math inline">\(\sigma^2\)</span> is the variance).</p>
<p>Do this preprocessing and repeat the training from above. You can use the Matlab function <code>normalize()</code> for this purpose. How are the results?</p>
<p>In Matlab, this can be achieved with the function <code>zscore()</code> or <code>normalize()</code> (depending on Matlab version).</p>
<p>Task: normalize the input data and recompute. From now on, use only the normalized data.</p>
<ul>
<li><strong>Question</strong>: According to the new model parameters, which is the most important factor in determining the perceived quality of the wine?</li>
</ul>
<p><strong>Task 4</strong>: Compute the solution with the Matlab function <code>fitml()</code> and check that it is the same</p>
<pre class="{matlab}"><code>%======================
% Your code here
%======================</code></pre>
<p><strong>Task 5</strong>: Compute the solution with the graphical tool Apps -&gt; Regression Learner</p>
<pre class="{matlab}"><code>%======================
% Your code here
%======================</code></pre>
<p><strong>Task 5</strong>: Implement optimization with Gradient Descent</p>
<pre class="{matlab}"><code>%======================
% To fill in
%======================

W = randn(12, 1);           % initialize parameters randomly

number_of_epochs = 1000;    % set number of iterations

for iter = 1:number_of_epochs

    % Compute predictions:
    Ypred = ...

    % Compute cost:
    J(iter) = 1/N * ...

    % Compute derivatives according to the given formula
    dW = ...

    % Update the weights
    mu = 0.0001;           % try multiple values here
    W = W - mu * dW;

    % Store the weights history
    W_hist(:,iter) = W;
end

% Plot the error and the evolution of the weights
plot(J)
figure
plot(W_hist)</code></pre>
<p><strong>Task 6</strong>: Create new features</p>
<p>The linear model tries to estimate the output only as a linear combination of the inputs <span class="math inline">\(x_i\)</span>. But what if the output depends, say, on the squared value <span class="math inline">\(x_i^2\)</span> of an input? The model cannot capture this. Instead, we can create new features manually, by performing transformations of the input data.</p>
<p>Augment the existing dataset by appending, for every column <span class="math inline">\(x_i\)</span>, new columns with the squared values <span class="math inline">\(x_i^2\)</span>, and also all possible products <span class="math inline">\(x_i \cdot x_j\)</span>. Then repeat the training process on the augmented data. How are the results?</p>
<p><strong>Task 7</strong>: Change dataset</p>
<p>Perform linear regression on the dataset available in “Admission_Predict_Ver1.1.csv”, using one of the methods described above (your choice).</p>
<p>Read the description of the dataset from it’s original homepage: <a href="https://www.kaggle.com/datasets/mohansacharya/graduate-admissions/">https://www.kaggle.com/datasets/mohansacharya/graduate-admissions/</a>.</p>
</section>
<section id="final-questions" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Final questions</h1>
<ol type="1">
<li><p>In Gradient Descent, what happens when <span class="math inline">\(\mu\)</span> is too large? What if it is too small?</p></li>
<li><p>What’s the difference between a coefficient <span class="math inline">\(w_i\)</span> being positive vs negative?</p></li>
</ol>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>