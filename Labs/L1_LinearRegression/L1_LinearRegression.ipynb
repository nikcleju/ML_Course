{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ML Laboratory 01: Linear Regression\n",
        "\n",
        "# 1. Objective\n",
        "\n",
        "Students should understand and be able use a linear regression model in\n",
        "Matlab\n",
        "\n",
        "# 2. Theoretical aspects\n",
        "\n",
        "## 2.1 Regression vs Classification\n",
        "\n",
        "A typical job in Machine Learning is to **predict** an output $y$ from\n",
        "some given data $(x_1, ... x_n)$:\n",
        "\n",
        "$$X = \\begin{bmatrix} x_1 & x_2 & \\dots & x_N \\end{bmatrix} \\rightarrow y$$\n",
        "\n",
        "In **supervised learning**, we are given many examples (input-output\n",
        "pairs) out of which we need to deduce the prediction rule.\n",
        "\n",
        "Depending on the meaning of $y$, we can have:\n",
        "\n",
        "-   **classification**: $y$ is a number representing a category (0 =\n",
        "    cat, 1 = dog, 2 = bird). The numbers have no real meaning as\n",
        "    numbers, whey are just numerical labels for representing the\n",
        "    categories.\n",
        "-   **regression**: $y$ is a number which actually means a numerical\n",
        "    result (it is the price of a house, or a probability, etc.).\n",
        "    Regression is also known as “curve fitting”.\n",
        "\n",
        "## 2.2 Linear regression: the model\n",
        "\n",
        "The linear regression model: the output is assumed to be a **linear\n",
        "combination** of the inputs.\n",
        "\n",
        "$$y \\approx w_1 x_1 + w_2 x_2 + ... + w_N x_N + b.$$\n",
        "\n",
        "The coefficients $w_i$ and $b$ are parameters we need to estimate/find.\n",
        "“Learning” means finding good values for the parameters, which get the\n",
        "job done.\n",
        "\n",
        "## 2.3 Linear regression: the parameters\n",
        "\n",
        "The parameters of the linear regression model are the **weights**\n",
        "$w_1, w_2, ... w_N$ and the **bias** value $b$ (also known as the\n",
        "**intercept**).\n",
        "\n",
        "## 2.4 Cost function (loss function)\n",
        "\n",
        "Given some parameters $w_i$ and $b$, how do we know if they are good?\n",
        "\n",
        "For an input vector $X$:\n",
        "\n",
        "-   we compute the **prediction**:\n",
        "    $$\\hat{y} = w_1 x_1 + w_2 x_2 + ... + w_N x_N + b$$\n",
        "-   we compare the prediction against the true result\n",
        "    (“**ground-truth**”) with the **cost function** (also known as\n",
        "    **loss function**): $$J = (\\hat{y} - y)^2$$\n",
        "\n",
        "The cost function defines what is good and what is bad, depending on its\n",
        "result (the cost):\n",
        "\n",
        "-   cost is small =\\> prediction is good\n",
        "-   cost is big =\\> prediction is bad\n",
        "\n",
        "If we have many data (input-output pairs), the overall cost is the\n",
        "average of the cost for each entry:\n",
        "$$J = \\frac{1}{N} \\sum_i (\\hat{y}^i - y^i)^2$$\n",
        "\n",
        "The cost function can be anything. Here, and typically for linear\n",
        "regression, we have the **quadratic cost function** (also known as\n",
        "“least squares”, “$\\ell_2$ norm”, …). This is typical good cost function\n",
        "for regression, but not so good for classification problems.\n",
        "$$(\\hat{y} - y)^2$$\n",
        "\n",
        "Other cost functions can be used, and they lead to different results\n",
        "(sometimes better, sometimes worse, depending on the problem).\n",
        "\n",
        "## 2.5 Matrix form of linear regression\n",
        "\n",
        "The linear regression problem can be written in matrix form as follows:\n",
        "\n",
        "$$\\begin{bmatrix} y^1 \\\\ y^2 \\\\ \\vdots \\\\ y^N\\end{bmatrix}\n",
        "\\approx\n",
        "\\begin{bmatrix} \\hat{y}^1 \\\\ \\hat{y}^2 \\\\ \\vdots \\\\\\hat{y}^N\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "x_1^1 & x_2^1 & x_3^1 & ... & x_{11}^1 & 1 \\\\\n",
        "x_1^2 & x_2^2 & x_3^2 & ... & x_{11}^2 & 1 \\\\\n",
        "\\vdots & \\vdots & \\vdots & ... & \\vdots & \\vdots \\\\\n",
        "x_1^N & x_2^N & x_3^N & ... & x_{11}^N & 1 \\\\\n",
        "\\end{bmatrix}\n",
        "\\cdot\n",
        "\\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_N \\\\ b \\end{bmatrix}$$\n",
        "\n",
        "Naming the matrices and vectors as $Y$, $\\hat{Y}$, $X$, $W$, we have:\n",
        "$$Y \\approx \\hat{Y} = X \\cdot W$$\n",
        "\n",
        "Note two important things:\n",
        "\n",
        "-   We can treat $b$ just like another weight, which multiplies some\n",
        "    constant value 1. We can extend the input matrix with a column of\n",
        "    1’s, and consider $b$ just like the 12-th weight in $W$.\n",
        "-   The same weights appear in all linear combinations. They are the\n",
        "    unknowns in this linear equation system.\n",
        "-   This is a **massively overcomplete** equation system. There is\n",
        "    probably no exact solution, but there exists a **least-squares\n",
        "    solution**, i.e. the solution vector $W$ which makes the predictions\n",
        "    $\\hat{Y}$ **as close as possible** to the true values $Y$\n",
        "    (i.e. minimum cost).\n",
        "\n",
        "## 2.6 How to train linear regression?\n",
        "\n",
        "**Training** = **learning** = finding good values for the unknown\n",
        "parameters.\n",
        "\n",
        "For linear regression, we can do it in three ways:\n",
        "\n",
        "### A. Closed form solution\n",
        "\n",
        "When the cost function is the quadratic, the best solution can actually\n",
        "be found analytically (this may be the only such case in the whole of\n",
        "Machine Learning :) ):\n",
        "\n",
        "$$W_{optimal} = X^\\dagger Y = \\left( X^* X \\right)^{-1} X^* \\cdot Y$$\n",
        "\n",
        "This is not true anymore if we change the cost function.\n",
        "\n",
        "The sign `*` signifies **transposition**.\n",
        "\n",
        "### B. Matlab function doing the job for us\n",
        "\n",
        "Linear regression can be fitted in Matlab using the function `fitml()`:\n",
        "\n",
        "``` {matlab}\n",
        "mdl = fitlm(X,Y)  % X are the inputs, Y is the target vector, mdl is a model object\n",
        "```\n",
        "\n",
        "### C. Matlab GUI App doing the job for us\n",
        "\n",
        "Matlab has a nice GUI tool for regression estimation in Apps -\\>\n",
        "Regression Learner.\n",
        "\n",
        "Play around with the tool and fit a linear regression model to the data\n",
        "\n",
        "### D. Optimization with Gradient Descent\n",
        "\n",
        "The cost function $J$ is a function like any other, and it depends on\n",
        "the parameters $w_i$.\n",
        "\n",
        "We can compute the derivative of $J$ with respect to each parameter,\n",
        "$\\frac{d J}{d w_i}$.\n",
        "\n",
        "The derivative tells us how the cost $J$ changes for a small increase in\n",
        "the parameter $w_i$. We want to reduce the cost function. If the\n",
        "derivative is positive, we’ll make $w_i$ a little smaller. If it is\n",
        "negative, we’ll increase it a bit.\n",
        "\n",
        "**Gradient Descent optimization procedure**:\n",
        "\n",
        "1.  set $\\mu$ = step size = small (e.g. 0.001)\n",
        "\n",
        "2.  initialize parameters $w_i$ somehow (random)\n",
        "\n",
        "3.  Repeat:\n",
        "\n",
        "    -   compute predictions and cost J\n",
        "    -   compute derivative of cost with respect to parameters\n",
        "        $\\frac{d J}{d w_i}$.\n",
        "    -   update each parameter like:\n",
        "        $$ w_i = w_i - \\mu \\cdot \\frac{d J}{d w_i}$$\n",
        "\n",
        "If we group all derivatives in a vector, this is known as the\n",
        "**gradient** vector:\n",
        "$$\\nabla W = \\begin{bmatrix} \\frac{d J}{d w_1} & \\frac{d J}{d w_2} & ... & \\frac{d J}{d w_{k}}\\end{bmatrix}.$$\n",
        "\n",
        "In matrix form, the update rule can be written as:\n",
        "$$W = W - \\mu \\nabla W.$$\n",
        "\n",
        "For linear regression with the quadratic cost function, it can be shown\n",
        "that the gradient is equal to: $$\\nabla W = X^* (\\hat{Y} - Y)$$\n",
        "\n",
        "# 3. Practical work\n",
        "\n",
        "The data used in this example comes from here:\n",
        "https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009\n",
        "\n",
        "The data contains 11 numerical chemical measurements for some different\n",
        "brands of red wines, together with a quality score indicated by buyers.\n",
        "The job is to determine how do the measured parameters influence the\n",
        "quality score. The ultimate goal is to predict the quality for some new\n",
        "type of red wine, based on its measurements.\n",
        "\n",
        "Inputs:\n",
        "\n",
        "-   1 - fixed acidity\n",
        "-   2 - volatile acidity\n",
        "-   3 - citric acid\n",
        "-   4 - residual sugar\n",
        "-   5 - chlorides\n",
        "-   6 - free sulfur dioxide\n",
        "-   7 - total sulfur dioxide\n",
        "-   8 - density\n",
        "-   9 - pH\n",
        "-   10 - sulphates\n",
        "-   11 - alcohol\n",
        "\n",
        "Outputs:\n",
        "\n",
        "-   12 - quality\n",
        "\n",
        "Let’s load the data first:\n",
        "\n",
        "``` {matlab}\n",
        "Data = readmatrix('winequality-red.csv');\n",
        "X = Data(:,1:11);   % 11 columns for the inputs\n",
        "Y = Data(:,13);     % 1 column for the output\n",
        "N = size(Data,1);   % The number of wines in the set (1599)\n",
        "```\n",
        "\n",
        "Extend the X matrix so we can treat the bias $b$ as just another weight.\n",
        "\n",
        "``` {matlab}\n",
        "X2 = [X ones(N,1)]\n",
        "```\n",
        "\n",
        "Let’s initialize the weights to some random values\n",
        "\n",
        "``` {matlab}\n",
        "W = randn(12, 1)   % a column vector\n",
        "```\n",
        "\n",
        "**Task 1**: Compute and show the cost function with the above weights\n",
        "\n",
        "``` {matlab}\n",
        "%======================\n",
        "% Your code here\n",
        "%======================\n",
        "```\n",
        "\n",
        "**Task 2**: Compute the solution with the closed-form formula\n",
        "\n",
        "-   **Question**: According to the model parameters, which is the most\n",
        "    important factor in determining the perceived quality of the wine?\n",
        "\n",
        "``` {matlab}\n",
        "%======================\n",
        "% Your code here\n",
        "%======================\n",
        "```\n",
        "\n",
        "**Task 3**: Repeat with normalized data\n",
        "\n",
        "The above question can be misleading, because our data may have\n",
        "different scales. To have a fair comparison, and to help the numerical\n",
        "algorithm, it’s recommended that the data is **normalized**.\n",
        "\n",
        "Normalization means, in this context, to make each column (each feature)\n",
        "have mean = 0 and standard deviation (or variance) = 1. This is achieved\n",
        "with the following transformation:\n",
        "\n",
        "$$x \\rightarrow \\frac{x - \\mu}{\\sigma},$$\n",
        "\n",
        "where $\\mu$ is the mean and $\\sigma$ is the standard deviation of every\n",
        "input column ($\\sigma^2$ is the variance).\n",
        "\n",
        "Do this preprocessing and repeat the training from above. You can use\n",
        "the Matlab function `normalize()` for this purpose. How are the results?\n",
        "\n",
        "In Matlab, this can be achieved with the function `zscore()` or\n",
        "`normalize()` (depending on Matlab version).\n",
        "\n",
        "Task: normalize the input data and recompute. From now on, use only the\n",
        "normalized data.\n",
        "\n",
        "-   **Question**: According to the new model parameters, which is the\n",
        "    most important factor in determining the perceived quality of the\n",
        "    wine?\n",
        "\n",
        "**Task 4**: Compute the solution with the Matlab function `fitml()` and\n",
        "check that it is the same\n",
        "\n",
        "``` {matlab}\n",
        "%======================\n",
        "% Your code here\n",
        "%======================\n",
        "```\n",
        "\n",
        "**Task 5**: Compute the solution with the graphical tool Apps -\\>\n",
        "Regression Learner\n",
        "\n",
        "``` {matlab}\n",
        "%======================\n",
        "% Your code here\n",
        "%======================\n",
        "```\n",
        "\n",
        "**Task 5**: Implement optimization with Gradient Descent\n",
        "\n",
        "``` {matlab}\n",
        "%======================\n",
        "% To fill in\n",
        "%======================\n",
        "\n",
        "W = randn(12, 1);           % initialize parameters randomly\n",
        "\n",
        "number_of_epochs = 1000;    % set number of iterations\n",
        "\n",
        "for iter = 1:number_of_epochs\n",
        "\n",
        "    % Compute predictions:\n",
        "    Ypred = ...\n",
        "\n",
        "    % Compute cost:\n",
        "    J(iter) = 1/N * ...\n",
        "\n",
        "    % Compute derivatives according to the given formula\n",
        "    dW = ...\n",
        "\n",
        "    % Update the weights\n",
        "    mu = 0.0001;           % try multiple values here\n",
        "    W = W - mu * dW;\n",
        "\n",
        "    % Store the weights history\n",
        "    W_hist(:,iter) = W;\n",
        "end\n",
        "\n",
        "% Plot the error and the evolution of the weights\n",
        "plot(J)\n",
        "figure\n",
        "plot(W_hist)\n",
        "```\n",
        "\n",
        "**Task 6**: Create new features\n",
        "\n",
        "The linear model tries to estimate the output only as a linear\n",
        "combination of the inputs $x_i$. But what if the output depends, say, on\n",
        "the squared value $x_i^2$ of an input? The model cannot capture this.\n",
        "Instead, we can create new features manually, by performing\n",
        "transformations of the input data.\n",
        "\n",
        "Augment the existing dataset by appending, for every column $x_i$, new\n",
        "columns with the squared values $x_i^2$, and also all possible products\n",
        "$x_i \\cdot x_j$. Then repeat the training process on the augmented data.\n",
        "How are the results?\n",
        "\n",
        "**Task 7**: Change dataset\n",
        "\n",
        "Perform linear regression on the dataset available in\n",
        "“Admission_Predict_Ver1.1.csv”, using one of the methods described above\n",
        "(your choice).\n",
        "\n",
        "Read the description of the dataset from it’s original homepage:\n",
        "<https://www.kaggle.com/datasets/mohansacharya/graduate-admissions/>.\n",
        "\n",
        "# 4. Final questions\n",
        "\n",
        "1.  In Gradient Descent, what happens when $\\mu$ is too large? What if\n",
        "    it is too small?\n",
        "\n",
        "2.  What’s the difference between a coefficient $w_i$ being positive vs\n",
        "    negative?"
      ],
      "id": "5307674c-df83-43eb-a59e-1da8cc586157"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    }
  }
}