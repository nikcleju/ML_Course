{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Laboratory 01: Linear Regression\n",
    "\n",
    "\n",
    "## 1. Objective\n",
    "\n",
    "Students should understand and be able use a linear regression model in Matlab\n",
    "\n",
    "## 2. Theoretical aspects\n",
    "\n",
    "### Regression vs Classification\n",
    "\n",
    "A typical job in Machine Learning is to **predict** an output $y$ from some given data $(x_1, ... x_n)$:\n",
    "\n",
    "$$X = \\begin{bmatrix} x_1 & x_2 & \\dots & x_N \\end{bmatrix} \\rightarrow y$$\n",
    "\n",
    "In **supervised learning**, we are given many examples (input-output pairs) out of which we need to deduce the prediction rule.\n",
    "\n",
    "Depending on the meaning of $y$, we can have:\n",
    "\n",
    "- **classification**: $y$ is a number representing a category (0 = cat, 1 = dog, 2 = bird). The numbers have no real meaning as numbers, whey are just numerical labels for representing the categories.\n",
    "\n",
    "- **regression**: $y$ is a number which actually means a numerical result (it is the price of a house, or a probability, etc.). Regression is also known as \"curve fitting\".\n",
    "\n",
    "### Linear regression: the model\n",
    "\n",
    "The linear regression model: the output is assumed to be a **linear combination** of the inputs.\n",
    "\n",
    "$$y \\approx w_1 x_1 + w_2 x_2 + ... + w_N x_N + b.$$\n",
    "\n",
    "The coefficients $w_i$ and $b$ are parameters we need to estimate/find. \"Learning\" means finding good values for the parameters, which get the job done.\n",
    "\n",
    "### Linear regression: the parameters\n",
    "\n",
    "The parameters of the linear regression model are the **weights** $w_1, w_2, ... w_N$ and the **bias** value $b$ (also known as the **intercept**).\n",
    "\n",
    "### Cost function (loss function)\n",
    "\n",
    "Given some parameters $w_i$ and $b$, how do we know if they are good? \n",
    "\n",
    "For an input vector $X$:\n",
    "\n",
    "- we compute the **prediction**:\n",
    "  $$\\hat{y} = w_1 x_1 + w_2 x_2 + ... + w_N x_N + b$$\n",
    "  \n",
    "- we compare the prediction against the true result (\"**ground-truth**\") with the **cost function** (also known as **loss function**):\n",
    "  $$J = (\\hat{y} - y)^2$$\n",
    "\n",
    "The cost function defines what is good and what is bad, depending on its result (the cost):\n",
    "  - cost is small => prediction is good\n",
    "  - cost is big => prediction is bad\n",
    "\n",
    "If we have many data (input-output pairs), the overall cost is the average of the cost for each entry:\n",
    "\n",
    "$$J = \\frac{1}{N} \\sum_i (\\hat{y}^i - y^i)^2$$\n",
    "\n",
    "The cost function can be anything. Here, and typically for linear regression, we have the **quadratic cost function** (also known as \"least squares\", \"$\\ell_2$ norm\", ...). This is typical good cost function for regression, but not so good for classification problems.\n",
    "$$(\\hat{y} - y)^2$$\n",
    "\n",
    "Other cost functions can be used, and they lead to different results (sometimes better, sometimes worse, depending on the problem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix form of linear regression\n",
    "\n",
    "The linear regression problem can be written in matrix form as follows:\n",
    "\n",
    "$$\\begin{bmatrix} y^1 \\\\ y^2 \\\\ \\vdots \\\\ y^N\\end{bmatrix}\n",
    "\\approx\n",
    "\\begin{bmatrix} \\hat{y}^1 \\\\ \\hat{y}^2 \\\\ \\vdots \\\\\\hat{y}^N\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "x_1^1 & x_2^1 & x_3^1 & ... & x_{11}^1 & 1 \\\\\n",
    "x_1^2 & x_2^2 & x_3^2 & ... & x_{11}^2 & 1 \\\\\n",
    "\\vdots & \\vdots & \\vdots & ... & \\vdots & \\vdots \\\\\n",
    "x_1^N & x_2^N & x_3^N & ... & x_{11}^N & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_N \\\\ b \\end{bmatrix}$$\n",
    "\n",
    "Naming the matrices and vectors as $Y$, $\\hat{Y}$, $X$, $W$, we have:\n",
    "$$Y \\approx \\hat{Y} = X \\cdot W$$\n",
    "\n",
    "Note two important things:\n",
    "\n",
    "- We can treat $b$ just like another weight, which multiplies some constant value 1. We can extend the input matrix with a column of 1's, and consider $b$ just like the 12-th weight in $W$. \n",
    "- The same weights appear in all linear combinations. They are the unknowns in this linear equation system.\n",
    "- This is a **massively overcomplete** equation system. There is probably no exact solution, but there exists a **least-squares solution**, i.e. the solution vector $W$ which makes the predictions $\\hat{Y}$ **as close as possible** to the true values $Y$ (i.e. minimum cost)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to train linear regression?\n",
    "\n",
    "**Training** = **learning** = finding good values for the unknown parameters.\n",
    "\n",
    "For linear regression, we can do it in three ways:\n",
    "\n",
    "#### 1. Closed form solution\n",
    "\n",
    "When the cost function is the quadratic, the best solution can actually be found analytically (this may be the only such case in the whole of Machine Learning :) ):\n",
    "\n",
    "$$W_{optimal} = X^\\dagger Y = \\left( X^* X \\right)^{-1} X^* \\cdot Y$$\n",
    "\n",
    "This is not true anymore if we change the cost function.\n",
    "\n",
    "The sign `*` signifies **transposition**. \n",
    "\n",
    "#### 2. Matlab function doing the job for us\n",
    "\n",
    "Linear regression can be fitted in Matlab using the function `fitml()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = fitlm(X,Y)  % X are the inputs, Y is the target vector, mdl is a model object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Optimization with Gradient Descent\n",
    "\n",
    "The cost function $J$ is a function like any other, and it depends on the parameters $w_i$.\n",
    "\n",
    "We can compute the derivative of $J$ with respect to each parameter, $\\frac{d J}{d w_i}$.\n",
    "\n",
    "The derivative tells us how the cost $J$ changes for a small increase in the parameter $w_i$. We want to reduce the cost function. If the derivative is positive, we'll make $w_i$ a little smaller. If it is negative, we'll increase it a bit.\n",
    "\n",
    "**Gradient Descent optimization procedure**:\n",
    "1. set $\\mu$ = step size = small (e.g. 0.001)\n",
    "2. initialize parameters $w_i$ somehow (random)\n",
    "2. Repeat:\n",
    "  - compute predictions and cost J\n",
    "  - compute derivative of cost with respect to parameters $\\frac{d J}{d w_i}$.\n",
    "  - update each parameter like:\n",
    "     $$ w_i = w_i - \\mu \\cdot \\frac{d J}{d w_i}$$\n",
    "     \n",
    "If we group all derivatives in a vector, this is known as the **gradient** vector:\n",
    "$$\\nabla W = \\begin{bmatrix} \\frac{d J}{d w_1} & \\frac{d J}{d w_2} & ... & \\frac{d J}{d w_{k}}\\end{bmatrix}.$$\n",
    "\n",
    "In matrix form, the update rule can be written as:\n",
    "$$W = W - \\mu \\nabla W.$$\n",
    "\n",
    "For linear regression with the quadratic cost function, it can be shown that the gradient is equal to:\n",
    "$$\\nabla W = X^* (\\hat{Y} - Y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Practical work\n",
    "\n",
    "The data used in this example comes from here: https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009\n",
    "\n",
    "The data contains 11 numerical chemical measurements for some different brands of red wines, together with a quality score indicated by buyers.\n",
    "The job is to determine how do the measured parameters influence the quality score. The ultimate goal is to predict the quality for some new type of red wine, based on its measurements.\n",
    "\n",
    "Inputs:\n",
    "\n",
    "   - 1 - fixed acidity\n",
    "   - 2 - volatile acidity\n",
    "   - 3 - citric acid\n",
    "   - 4 - residual sugar\n",
    "   - 5 - chlorides\n",
    "   - 6 - free sulfur dioxide\n",
    "   - 7 - total sulfur dioxide\n",
    "   - 8 - density\n",
    "   - 9 - pH\n",
    "   - 10 - sulphates\n",
    "   - 11 - alcohol \n",
    "   \n",
    "Outputs:\n",
    "\n",
    "   - 12 - quality\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = readmatrix('winequality-red.csv');\n",
    "X = Data(:,1:11);   % 11 columns for the inputs\n",
    "Y = Data(:,12);     % 1 column for the output\n",
    "N = size(Data,1);   % The number of wines in the set (1599)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend the X matrix so we can treat the bias $b$ as just another weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [X ones(N,1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the weights to some random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = randn(12, 1)   % a column vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**: Compute and show the cost function with the above weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%======================\n",
    "% Your code here\n",
    "%======================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Compute the solution with the closed-form formula\n",
    "  - **Question**: Which is the most important factor in determining the perceived quality of the wine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%======================\n",
    "% Your code here\n",
    "%======================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Compute the solution with the Matlab function `fitml()` and check that it is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%======================\n",
    "% Your code here\n",
    "%======================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4**: Implement optimization with Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%======================\n",
    "% To fill in\n",
    "%======================\n",
    "\n",
    "W = randn(12, 1);           % initialize parameters randomly\n",
    "\n",
    "number_of_epochs = 1000;    % set number of iterations\n",
    "\n",
    "for iter = 1:number_of_epochs\n",
    "    \n",
    "    % Compute predictions:\n",
    "    Ypred = ...\n",
    "    \n",
    "    % Compute cost:\n",
    "    J(iter) = 1/N * ...\n",
    "    \n",
    "    % Compute derivatives according to the given formula\n",
    "    dW = ...\n",
    "    \n",
    "    % Update the weights\n",
    "    mu = 0.0001;           % try multiple values here\n",
    "    W = W - mu * dW;\n",
    "    \n",
    "    % Store the weights history\n",
    "    W_hist(:,iter) = W;\n",
    "end\n",
    "\n",
    "% Plot the error and the evolution of the weights\n",
    "plot(J)\n",
    "figure\n",
    "plot(W_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5**: Repeat, with normalized input data\n",
    "\n",
    "In many machine learning tasks, it is better to normalize the input data. Normalization means, in this context, to make each column (each feature) zero-mean and unit-variance, by subtracting the mean and dividing to the standard deviation:\n",
    "\n",
    "$$x \\rightarrow \\frac{x - \\mu}{\\sigma},$$\n",
    "\n",
    "where $\\mu$ is the mean and $\\sigma$ is the standard deviation of every input column ($\\sigma^2$ is the variance).\n",
    "\n",
    "Do this preprocessing and repeat the training from above. You can use the Matlab function `normalize()` for this purpose. How are the results?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6**: Create new features\n",
    "\n",
    "The linear model tries to estimate the output only as a linear combination of the inputs $x_i$. But what if the output depends, say, on the squared value $x_i^2$ of an input? The model cannot capture this. Instead, we can create new features manually, by performing transformations of the input data. \n",
    "\n",
    "Augment the existing dataset by appending, for every column $x_i$, new columns with the squared values $x_i^2$, and also all possible products $x_i \\cdot x_j$. Then repeat the training process on the augmented data. How are the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final questions\n",
    "\n",
    "1. What happens when $\\mu$ is too large? What if it is too small?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Matlab",
   "language": "matlab",
   "name": "matlab"
  },
  "language_info": {
   "codemirror_mode": "octave",
   "file_extension": ".m",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "matlab",
   "version": "0.16.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
