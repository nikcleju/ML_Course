<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>MultiLayer Perceptron (MLP) neural networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="L3_Multilayer_files/libs/clipboard/clipboard.min.js"></script>
<script src="L3_Multilayer_files/libs/quarto-html/quarto.js"></script>
<script src="L3_Multilayer_files/libs/quarto-html/popper.min.js"></script>
<script src="L3_Multilayer_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="L3_Multilayer_files/libs/quarto-html/anchor.min.js"></script>
<link href="L3_Multilayer_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="L3_Multilayer_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="L3_Multilayer_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="L3_Multilayer_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="L3_Multilayer_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#objective" id="toc-objective" class="nav-link active" data-scroll-target="#objective"><span class="header-section-number">1</span> Objective</a></li>
  <li><a href="#theoretical-aspects" id="toc-theoretical-aspects" class="nav-link" data-scroll-target="#theoretical-aspects"><span class="header-section-number">2</span> Theoretical aspects</a>
  <ul class="collapse">
  <li><a href="#multilayer-perceptron" id="toc-multilayer-perceptron" class="nav-link" data-scroll-target="#multilayer-perceptron"><span class="header-section-number">2.1</span> Multilayer perceptron</a>
  <ul class="collapse">
  <li><a href="#cascading-neurons" id="toc-cascading-neurons" class="nav-link" data-scroll-target="#cascading-neurons">Cascading neurons</a></li>
  <li><a href="#multiple-outputs" id="toc-multiple-outputs" class="nav-link" data-scroll-target="#multiple-outputs">Multiple outputs</a></li>
  <li><a href="#multiple-layers" id="toc-multiple-layers" class="nav-link" data-scroll-target="#multiple-layers">Multiple layers</a></li>
  <li><a href="#matrix-form" id="toc-matrix-form" class="nav-link" data-scroll-target="#matrix-form">Matrix form</a></li>
  </ul></li>
  <li><a href="#the-model" id="toc-the-model" class="nav-link" data-scroll-target="#the-model"><span class="header-section-number">2.2</span> The model</a></li>
  <li><a href="#the-model-parameters" id="toc-the-model-parameters" class="nav-link" data-scroll-target="#the-model-parameters"><span class="header-section-number">2.3</span> The model parameters</a></li>
  <li><a href="#the-cost-function" id="toc-the-cost-function" class="nav-link" data-scroll-target="#the-cost-function"><span class="header-section-number">2.4</span> The cost function</a></li>
  <li><a href="#training" id="toc-training" class="nav-link" data-scroll-target="#training"><span class="header-section-number">2.5</span> Training</a>
  <ul class="collapse">
  <li><a href="#backpropagation" id="toc-backpropagation" class="nav-link" data-scroll-target="#backpropagation">Backpropagation</a></li>
  </ul></li>
  <li><a href="#matlab-functions-for-working-with-neural-networks" id="toc-matlab-functions-for-working-with-neural-networks" class="nav-link" data-scroll-target="#matlab-functions-for-working-with-neural-networks"><span class="header-section-number">2.6</span> Matlab functions for working with neural networks</a>
  <ul class="collapse">
  <li><a href="#toy-example-setup" id="toc-toy-example-setup" class="nav-link" data-scroll-target="#toy-example-setup">Toy Example setup</a></li>
  <li><a href="#neural-net-tools" id="toc-neural-net-tools" class="nav-link" data-scroll-target="#neural-net-tools">Neural Net tools</a></li>
  <li><a href="#classification-learner" id="toc-classification-learner" class="nav-link" data-scroll-target="#classification-learner">Classification Learner</a></li>
  <li><a href="#the-patternnet-function" id="toc-the-patternnet-function" class="nav-link" data-scroll-target="#the-patternnet-function">The <code>patternnet()</code> function</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#practical-work" id="toc-practical-work" class="nav-link" data-scroll-target="#practical-work"><span class="header-section-number">3</span> Practical work</a>
  <ul class="collapse">
  <li><a href="#split" id="toc-split" class="nav-link" data-scroll-target="#split"><span class="header-section-number">3.1</span> Split</a></li>
  <li><a href="#exercise-0---manual-calculations" id="toc-exercise-0---manual-calculations" class="nav-link" data-scroll-target="#exercise-0---manual-calculations"><span class="header-section-number">3.2</span> Exercise 0 - Manual calculations</a></li>
  <li><a href="#exercise-1---wine-classification---nprtool" id="toc-exercise-1---wine-classification---nprtool" class="nav-link" data-scroll-target="#exercise-1---wine-classification---nprtool"><span class="header-section-number">3.3</span> Exercise 1 - Wine classification - nprtool</a></li>
  <li><a href="#exercise-2---wine-classification---classification-learner" id="toc-exercise-2---wine-classification---classification-learner" class="nav-link" data-scroll-target="#exercise-2---wine-classification---classification-learner"><span class="header-section-number">3.4</span> Exercise 2 - Wine classification - Classification Learner</a></li>
  <li><a href="#exercise-3---handwritten-digit-classification-on-mnist" id="toc-exercise-3---handwritten-digit-classification-on-mnist" class="nav-link" data-scroll-target="#exercise-3---handwritten-digit-classification-on-mnist"><span class="header-section-number">3.5</span> Exercise 3 - Handwritten digit classification on MNIST</a></li>
  <li><a href="#exercise-4---classifying-wheat-seeds-independent-work" id="toc-exercise-4---classifying-wheat-seeds-independent-work" class="nav-link" data-scroll-target="#exercise-4---classifying-wheat-seeds-independent-work"><span class="header-section-number">3.6</span> Exercise 4 - Classifying wheat seeds (independent work)</a></li>
  </ul></li>
  <li><a href="#final-questions" id="toc-final-questions" class="nav-link" data-scroll-target="#final-questions"><span class="header-section-number">4</span> Final questions</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="L3_Multilayer.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li><li><a href="L3_Multilayer.ipynb" download="L3_Multilayer.ipynb"><i class="bi bi-journal-code"></i>Jupyter</a></li></ul></div></nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">MultiLayer Perceptron (MLP) neural networks</h1>
<p class="subtitle lead">ML Laboratory 03</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="objective" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Objective</h1>
<p>Students should understand and be able use a multi-layer fully-connected networks in Matlab</p>
</section>
<section id="theoretical-aspects" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Theoretical aspects</h1>
<p>Multi-layer perceptron (fully-connected) neural networks are widely used for classification of small, simple datasets.</p>
<section id="multilayer-perceptron" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="multilayer-perceptron"><span class="header-section-number">2.1</span> Multilayer perceptron</h2>
<section id="cascading-neurons" class="level3">
<h3 class="anchored" data-anchor-id="cascading-neurons">Cascading neurons</h3>
<p>Logistic regression = 1 neuron</p>
<p>A single neuron creates a single hyperplane and separates the input space in two categories 0 or 1)</p>
<ul>
<li><strong>“neuron”</strong> = one logistic regression operation</li>
<li><strong>“hyperplane”</strong> = a linear boundary surface, with dimension N-1</li>
<li>with a smooth sigmoid transition zone between the two classes</li>
</ul>
<p>What if we have a dataset as follows? How to do classification here?</p>
<!-- <div>
<img src=img/DatasetAngled1.png align="center" width="200"/>
</div> -->
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/DatasetAngled1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">How to separate the classes in this dataset?</figcaption>
</figure>
</div>
<p>Solution: use <strong>two neurons</strong>:</p>
<ul>
<li>each one draws a hyperplane (a line)</li>
<li>aggregate their results into the final outcome: “When both neurons say 1, output class is 1. Otherwise, output class is 0”.</li>
</ul>
<!--
<div>
<img src=img/DatasetAngled2.png align="center" width="400"/>
</div> -->
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/DatasetAngled2.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Each neuron draws a line…</figcaption>
</figure>
</div>
<p>Combining the results of both neurons in the final result is <strong>also done with a (third) neuron</strong>. Thus, we have <strong>cascading neurons</strong>.</p>
<!--
<div>
<img src=img/Network2plus1.png align="center" width="500"/>
</div> -->
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/Network2plus1.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">.. and a third neuron combines them</figcaption>
</figure>
</div>
<p>Neurons operating on the same inputs form a <strong>layer</strong>. We have two layers now:</p>
<ul>
<li>The inputs (this does not contain neurons, just the inputs, but it is commonly named “the input layer”)</li>
<li>The hidden layer (middle)</li>
<li>The output layer (the output neuron)</li>
</ul>
<p>What if we want a boundary composed of 3 sides? Use three neurons in the hidden layer.</p>
<p>What if we want a curved boundary? Use many more neurons (approximate the curve from many lines)</p>
<p><strong>Any hypersurface</strong> can be obtained with just two layers, provided there are enough neurons in the hidden layer:</p>
<ol type="1">
<li>The hidden layer draws some hyperplanes (e.g.&nbsp;lines)</li>
<li>The output layer combines the results into output values</li>
</ol>
</section>
<section id="multiple-outputs" class="level3">
<h3 class="anchored" data-anchor-id="multiple-outputs">Multiple outputs</h3>
<p>What if we have 4 output classes?</p>
<p>Have 4 neurons in the output layer, one for each class. When the input belongs to class <span class="math inline">\(k\)</span>, the <span class="math inline">\(k\)</span>-th neuron should produce 1, and all the others should produce 0.</p>
<p><strong>One-hot encoding</strong>: When we train the network, we need to tell it what is the desired output (target). This is known as <strong>encoding</strong>. For an input of class <span class="math inline">\(k\)</span>, we tell the network to produce a vector with a single value of 1, on position <span class="math inline">\(k\)</span>. <span class="math display">\[\begin{bmatrix} 0 \\ \vdots \\ 0 \\ 1 \\ 0 \\ \vdots \end{bmatrix}\]</span></p>
<p>After training, when running the model, we look at <strong>the location</strong> of the highest value and the location is the predicted class.</p>
</section>
<section id="multiple-layers" class="level3">
<h3 class="anchored" data-anchor-id="multiple-layers">Multiple layers</h3>
<p>We can actually have more than 2 layers in a network. We can have as many as we want! Interpretation:</p>
<ul>
<li>first hidden layer draws some hyperplanes</li>
<li>next layer combines hyperplanes into some simpler shapes</li>
<li>next layer combines the simple shapes into more complex shapes</li>
<li>….</li>
<li>final layer gives the output</li>
</ul>
<p>In practice, it is often better to have <strong>more layers with fewer neurons</strong> than 2 layers but with a huge hidden layer.</p>
<p>However, training many layers and many neurons is <strong>difficult</strong>, i.e.&nbsp;it can overfit, become unstable, etc.</p>
</section>
<section id="matrix-form" class="level3">
<h3 class="anchored" data-anchor-id="matrix-form">Matrix form</h3>
<p><strong>One neuron</strong> does a linear combination of the inputs, followed by activation function: <span class="math display">\[\begin{bmatrix} w_1 &amp; x_w &amp; \dots &amp; w_N &amp; b \end{bmatrix} \cdot \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_N \\ 1 \end{bmatrix} = z
\rightarrow \sigma(z) = a\]</span></p>
<p><span class="math display">\[\begin{bmatrix} w_1 &amp; x_w &amp; \dots &amp; w_N \end{bmatrix} \cdot \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_N \end{bmatrix}  + b = z
\rightarrow \sigma(z) = a\]</span></p>
<p><strong>A layer of <span class="math inline">\(M\)</span> neurons is just <span class="math inline">\(M\)</span> neurons next to each other</strong>: <span class="math display">\[\begin{bmatrix}
w_{11} &amp; w_{12} &amp; \dots &amp; w_{1N} &amp; b_1 \\
w_{21} &amp; w_{22} &amp; \dots &amp; w_{2N} &amp; b_2 \\
\vdots  &amp; \vdots &amp; \dots &amp; \vdots &amp; \vdots \\
w_{M1} &amp; w_{M2} &amp; \dots &amp; w_{MN} &amp; b_M \\
\end{bmatrix} \cdot \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_N \\ 1 \end{bmatrix} =
\begin{bmatrix}
z_1 \\ z_2 \\ \dots \\ z_M
\end{bmatrix}
\rightarrow
\sigma(\begin{bmatrix}
z_1 \\ z_2 \\ \dots \\ z_M
\end{bmatrix})
=
\begin{bmatrix}
a_1 \\ a_2 \\ \dots \\ a_M
\end{bmatrix}\]</span></p>
<p><span class="math display">\[\begin{bmatrix}
w_{11} &amp; w_{12} &amp; \dots &amp; w_{1N}\\
w_{21} &amp; w_{22} &amp; \dots &amp; w_{2N}\\
\vdots  &amp; \vdots &amp; \dots &amp; \vdots\\
w_{M1} &amp; w_{M2} &amp; \dots &amp; w_{MN}\\
\end{bmatrix} \cdot \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_N \end{bmatrix}
+
\begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_N \end{bmatrix}
=
\begin{bmatrix}
z_1 \\ z_2 \\ \dots \\ z_M
\end{bmatrix}
\rightarrow
\sigma(\begin{bmatrix}
z_1 \\ z_2 \\ \dots \\ z_M
\end{bmatrix})
=
\begin{bmatrix}
a_1 \\ a_2 \\ \dots \\ a_M
\end{bmatrix}\]</span></p>
<p>Each layer is characterized by the weight matrix <span class="math inline">\(W\)</span> and column <span class="math inline">\(b\)</span> (separately or joined).</p>
<p>The whole network can be understood as a sequence of matrix multiplications and activation functions.</p>
<p><strong>The next layer</strong> takes as inputs the outputs <span class="math inline">\(a_i\)</span> of the previous layer, and does the same.</p>
</section>
</section>
<section id="the-model" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="the-model"><span class="header-section-number">2.2</span> The model</h2>
<p>The multi-layer perceptron model contains <span class="math inline">\(L\)</span> layers, each layer consisting of a matrix multiplication and activation function:</p>
<p><span class="math display">\[\begin{aligned}
z^{1} =&amp; W^{1} \cdot X \\
a^{1} =&amp; activation(z^{1}) \\
\\
z^{2} =&amp; W^{2} \cdot a^{1} \\
a^{2} =&amp; activation(z^{2}) \\
... \\
z^{k} =&amp; W^{k} \cdot a^{k-1} \\
a^{k} =&amp; activation(z^{k})\\
\end{aligned}\]</span></p>
<p>Here, <span class="math inline">\(W^{k}\)</span> is a matrix and <span class="math inline">\(z{k}\)</span>, <span class="math inline">\(a{k}\)</span> are vectors (columns). <span class="math inline">\(k\)</span> refers to layer number <span class="math inline">\(k\)</span>.</p>
<p>The activation function can be the <strong>sigmoid</strong>, <strong>ReLU</strong>, <strong>tanh</strong> etc. Typically the outputs use sigmoid activation, but all others are up to the designer.</p>
<ul>
<li><p>sigmoid activation function: <span class="math display">\[f(z) = \sigma(z) = \frac{1}{1 + e^{-z}} = \frac{e^z}{e^z + 1}\]</span></p></li>
<li><p>ReLU (“Rectified Linear Unit”): <span class="math display">\[f(z) = \begin{cases} z &amp;, \textrm{ if } z \geq 0 \\ 0 &amp;,  \textrm{ if } z &lt; 0  \end{cases}\]</span></p></li>
<li><p>Hyperbolic tangent:</p>
<p>…</p></li>
</ul>
<p><strong>Inputs</strong>:</p>
<ul>
<li>a matrix <span class="math inline">\(X\)</span> with every input vector being a column (according to the equations below; we can also transpose all matrices and vectors, if we want).</li>
</ul>
<p><strong>Outputs</strong> (assuming one-hot encoding):</p>
<ul>
<li>a vector <span class="math inline">\(\hat{y}\)</span> which should be understood as scores/probability of belonging in each class</li>
<li>the <strong>location of the maximum</strong> value gives the predicted class</li>
</ul>
</section>
<section id="the-model-parameters" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="the-model-parameters"><span class="header-section-number">2.3</span> The model parameters</h2>
<p>The model parameters are the weight matrices <span class="math inline">\(W^{k}\)</span> of every layer. The element <span class="math inline">\(w^{k}_{ij}\)</span> is the weight in the <span class="math inline">\(k\)</span>-th layer, <span class="math inline">\(i\)</span>-th neuron, <span class="math inline">\(j\)</span>-th input of it.</p>
<p>Every neuron has a bias input. We presume that the bias is included in the weight matrices, like we did until now (e.g.&nbsp;like a fake input equal to 1 is appended to the input of every layer).</p>
</section>
<section id="the-cost-function" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="the-cost-function"><span class="header-section-number">2.4</span> The cost function</h2>
<p>For classification, the <strong>cross-entropy</strong> loss function is typically used. For a single input, whose output is a vector <span class="math inline">\(\hat{y} = [\hat{y}_1, ... \hat{y}_N]\)</span> (assuming N classes) and true output should have been <span class="math inline">\(y = [y_1, ... y_N]\)</span>, the cross-entropy function is defined as:</p>
<p><span class="math display">\[L(y, \hat{y}) = - y_1 \log_2{\hat{y_1}} - \dots - y_n \log_2{\hat{y_n}} = -\log_2{\hat{y_{class}}},\]</span></p>
<p>where <span class="math inline">\(\hat{y_{class}}\)</span> is the model’s predicted probability for the true class of the input. Note that we assume that the true output is one-hot encoded, i.e.&nbsp;the vector <span class="math inline">\(y\)</span> has a single value of 1 and all the others are 0.</p>
<p>For a binary classification, the same rule stands: the loss = - logarithm in base 2 of the probability of the true class. Since we have a single output <span class="math inline">\(y\)</span>, the probability of the true class is:</p>
<ul>
<li><span class="math inline">\(y\)</span>, if the true class is class 1</li>
<li><span class="math inline">\(1-y\)</span>, if the true class is class 0</li>
</ul>
<p>For several inputs (a “batch”), do the average of all losses: <span class="math display">\[J = \frac{1}{N} \sum_i L(y^i, \hat{y}^i)\]</span></p>
</section>
<section id="training" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="training"><span class="header-section-number">2.5</span> Training</h2>
<p>Training is done with <strong>backpropagation</strong> and gradient descent (or some variant of it).</p>
<p><strong>Backpropagation</strong> = the technique to compute the derivatives of <span class="math inline">\(J\)</span> with respect to all parameters in the network.</p>
<section id="backpropagation" class="level3">
<h3 class="anchored" data-anchor-id="backpropagation">Backpropagation</h3>
<p>Assume we have a network with 4 layers.</p>
<p><span class="math display">\[\begin{aligned}
z^{1} =&amp; W^{1} \cdot X \\
a^{1} =&amp; activation(z^{1}) \\
\\
z^{2} =&amp; W^{2} \cdot a^{1} \\
a^{2} =&amp; activation(z^{2}) \\
\\
z^{3} =&amp; W^{3} \cdot a^{2} \\
a^{3} =&amp; activation(z^{3}) \\
\\
z^{4} =&amp; W^{4} \cdot a^{3} \\
a^{4} =&amp; activation(z^{4})
\end{aligned}\]</span></p>
<p>The final results <span class="math inline">\(a^{4}\)</span> are the outputs <span class="math inline">\(a^{4} = \hat{y}\)</span>.</p>
<p>Just like in logistic regression, we can compute the derivatives for the final layer, <span class="math inline">\(\frac{dJ}{dW^{4}}\)</span> and <span class="math inline">\(\frac{dJ}{da^3}\)</span></p>
<p>For the third layer, we compute its own derivatives, <span class="math inline">\(\frac{da^{3}}{dW^{3}}\)</span> and <span class="math inline">\(\frac{da^{3}}{da^2}\)</span>. Together with the <span class="math inline">\(\frac{dJ}{da^3}\)</span> received as inputs from the above layer, we have:</p>
<p><span class="math display">\[\frac{dJ}{dW^{3}} = \frac{dJ}{da^3} \cdot \frac{da^{3}}{dW^{3}}\]</span> and <span class="math display">\[\frac{dJ}{da^{2}} = \frac{dJ}{da^3} \cdot \frac{da^{3}}{da^{2}}\]</span></p>
<p>For the second layer, we compute its own derivatives, <span class="math inline">\(\frac{da^{2}}{dW^{2}}\)</span> and <span class="math inline">\(\frac{da^{2}}{da^1}\)</span>. Together with the <span class="math inline">\(\frac{dJ}{da^2}\)</span> received as inputs from the above layer, we have:</p>
<p><span class="math display">\[\frac{dJ}{dW^{2}} = \frac{dJ}{da^2} \cdot \frac{da^{2}}{dW^{2}}\]</span> and <span class="math display">\[\frac{dJ}{da^{1}} = \frac{dJ}{da^1} \cdot \frac{da^{2}}{da^{1}}\]</span></p>
<p>Finally, the input layer computes its own derivatives, <span class="math inline">\(\frac{da^{1}}{dW^{1}}\)</span>, aand together with the <span class="math inline">\(\frac{dJ}{da^{1}}\)</span> received from the layer above, computes: <span class="math display">\[\frac{dJ}{dW^{1}} = \frac{dJ}{da^1} \cdot \frac{da^{1}}{dW^{1}}\]</span></p>
<p>In backpropagation, <strong>each layer (each operation, really) does the following</strong>:</p>
<ol type="1">
<li>Has some inputs I, parameters P, and outputs O.</li>
<li>Knows show to compute its own derivatives <span class="math inline">\(\frac{dO}{dP}\)</span> and <span class="math inline">\(\frac{dO}{dI}\)</span></li>
<li>Receives as input from the next layer the quantity <span class="math inline">\(\frac{dJ}{dO}\)</span></li>
<li>Computes <span class="math inline">\(\frac{dJ}{dP} = \frac{dJ}{dO} \cdot \frac{dO}{dP}\)</span>. This will be used in Gradient Descent.</li>
<li>Computes <span class="math inline">\(\frac{dJ}{dI} = \frac{dJ}{dI} \cdot \frac{dO}{dI}\)</span> and passes them back to the preceding layer.</li>
</ol>
<p>Backpropagation is a <strong>computational graph</strong> (sequence of operations) not unlike the model itself is just a sequence of operations. The only difference is that the data travels <strong>backwards</strong>, from the network output towards its input. The “data” here is the gradients (derivatives).</p>
<p>Training the model means repeating the two passes:</p>
<ol type="1">
<li><strong>Forward pass</strong>: run the model (from the inputs, and current parameters, compute the outputs and the cost function)</li>
<li><strong>Backward pass</strong>: backpropagation + gradient descent (from the cost function, compute gradients and update parameters, going backwards to the input</li>
<li>Repeat</li>
</ol>
<p>After the gradients are calculated, we can update the parameters.</p>
<p><strong>Gradient descent</strong> refers to the typical update rule <span class="math inline">\(W = W - \mu \frac{dJ}{dW}\)</span>. There exist also some smarter variations of it.</p>
</section>
</section>
<section id="matlab-functions-for-working-with-neural-networks" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="matlab-functions-for-working-with-neural-networks"><span class="header-section-number">2.6</span> Matlab functions for working with neural networks</h2>
<section id="toy-example-setup" class="level3">
<h3 class="anchored" data-anchor-id="toy-example-setup">Toy Example setup</h3>
<p>Let’s generate some data points belonging in 3 classes, situated around central points <code>(2,2)</code>, <code>(1,-1)</code> and <code>(-1,1)</code>:</p>
<p>We do <strong>one hot encoding</strong> of the output vector Y. In R2020b we can use the function <code>onehotencode()</code>. In previous versions, we can use the function <code>ind2vec()</code>.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode matlab code-with-copy"><code class="sourceCode matlab"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>[<span class="va">X</span><span class="op">,</span><span class="va">Y</span>] <span class="op">=</span> <span class="va">make_data</span>([<span class="fl">2</span> <span class="fl">2</span><span class="op">;</span>   <span class="fl">1</span> <span class="op">-</span><span class="fl">1</span><span class="op">;</span>    <span class="op">-</span><span class="fl">1</span><span class="op">,</span><span class="fl">1</span>])<span class="op">;</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="va">Yonehot</span> <span class="op">=</span> <span class="va">onehotencode</span>(<span class="va">categorical</span>(<span class="va">Y</span>)<span class="op">,</span><span class="fl">2</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="neural-net-tools" class="level3">
<h3 class="anchored" data-anchor-id="neural-net-tools">Neural Net tools</h3>
<ul>
<li>for <strong>classification</strong>: <code>nprtool()</code> (Neural Net Pattern Recognition)</li>
<li>for <strong>regression</strong>: <code>nftool()</code> (Neural Net Fitting)</li>
<li><code>nnstart()</code>: entry-point for both of the above</li>
</ul>
<ol type="1">
<li><p>Open <code>nprtool</code> and select input and output data. Outputs must be one-hot encoded.</p>
<p><img src="img/2022-11-26-20-09-22.png" class="img-fluid"></p></li>
<li><p>Select Training, Validation and Test set size.</p></li>
</ol>
<ul>
<li>The <strong>training set</strong> contains the data used for the actual training.</li>
<li>The <strong>validation set</strong> contains the data used to stop the training before over-fitting (over-learning). For a fair decision, for validation we use data which is not used for actual training.</li>
<li>The <strong>testing set</strong> contains the data used for the final testing of the final trained model. For a fair result, we use data which the network has never seen until now (neither in training nor for validation).</li>
</ul>
<p>Default values for small datasets are around 70% / 15% / 15%. For larger datasets (tens of thousands of input vectors), we can use smaller amounts for validation and testing.</p>
<p>These three sets of data are selected <strong>randomly</strong> from the overall available dataset, for fairness.</p>
<ol start="3" type="1">
<li><p>Select hidden layer size. Output layer size is determined by the number of output classes.</p>
<p>In general, pick an intermediate value between size of input and size of output, so that the network has an overall decreasing size towards the output.</p>
<p><img src="img/2022-11-26-20-12-11.png" class="img-fluid"></p></li>
<li><p>Run Train</p></li>
<li><p>Evaluate outputs: Error, Confusion matrix, etc.</p>
<p><img src="img/2022-11-26-20-15-18.png" class="img-fluid"></p></li>
<li><p>Click Next and deploy the neural network as a Matlab function</p>
<p><img src="img/2022-11-26-20-17-09.png" class="img-fluid"></p></li>
<li><p>Call the function for a new input. It returns the network output, i.e.&nbsp;the score for each class.</p>
<p>The higher score is the predicted class.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode matlab code-with-copy"><code class="sourceCode matlab"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="va">myNeuralNetworkFunction</span>([<span class="fl">1</span><span class="op">,</span><span class="fl">2</span>])</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="va">ans</span> <span class="op">=</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a> <span class="fl">0.8166</span>    <span class="fl">0.0117</span>    <span class="fl">0.1716</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ol>
</section>
<section id="classification-learner" class="level3">
<h3 class="anchored" data-anchor-id="classification-learner">Classification Learner</h3>
<ol type="1">
<li><p>Use the same Classification Learner tool introduced last time, and load the data.</p></li>
<li><p>Select a Neural Network classifier.</p>
<p><img src="img/2022-11-26-20-26-35.png" class="img-fluid"></p></li>
<li><p>You can select more advanced parameters (number of layers, size of layers, type of activation function)</p>
<p><img src="img/2022-11-26-20-28-39.png" class="img-fluid"></p></li>
<li><p>Inspect outputs (e.g.&nbsp;Confusion matrix)</p></li>
<li><p>Export model</p></li>
<li><p>Call the model’s predict functions:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode matlab code-with-copy"><code class="sourceCode matlab"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="va">trainedModel</span>.<span class="va">predictFcn</span>([<span class="op">-</span><span class="fl">1</span><span class="op">,</span><span class="fl">2</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ol>
</section>
<section id="the-patternnet-function" class="level3">
<h3 class="anchored" data-anchor-id="the-patternnet-function">The <code>patternnet()</code> function</h3>
<p>For more fine-grained control, you can use the <code>patternnet()</code> function. Read its documentation to see more details.</p>
<p><strong>Note</strong>: All the data vectors should be column vectors.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode matlab code-with-copy"><code class="sourceCode matlab"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">% Construct a pattern network with two hidden layer of sizes 8 and 4</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="va">net</span> <span class="op">=</span> <span class="va">patternnet</span>([<span class="fl">8</span> <span class="fl">4</span>])<span class="op">;</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co">% Train the network net using the training data (outputs are one-hot-encoded).</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co">% Use the default training algorithm.</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="va">net</span> <span class="op">=</span> <span class="va">train</span>(<span class="va">net</span><span class="op">,</span><span class="va">X</span><span class="op">',</span><span class="va">Yonehot</span><span class="op">'</span>)<span class="op">;</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co">% View the trained network.</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="va">view</span>(<span class="va">net</span>)<span class="op">;</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co">% Predict using the trained network. Pass a column vector!</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a><span class="va">y</span> <span class="op">=</span> <span class="va">net</span>([<span class="op">-</span><span class="fl">1</span><span class="op">;</span> <span class="op">-</span><span class="fl">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
</section>
<section id="practical-work" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Practical work</h1>
<section id="split" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="split"><span class="header-section-number">3.1</span> Split</h2>
<p>In Week 1:</p>
<ul>
<li>Exercise 1</li>
<li>Exercise 2</li>
<li>Exercise 3 (fast)</li>
</ul>
<p>In week 2, do/redo with greater care:</p>
<ul>
<li>Exercise 0</li>
<li>Exercise 3 - train and run on the test images as well</li>
<li>Exercise 4 - independently</li>
</ul>
</section>
<section id="exercise-0---manual-calculations" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="exercise-0---manual-calculations"><span class="header-section-number">3.2</span> Exercise 0 - Manual calculations</h2>
<p>Consider a neural network with the following architecture:</p>
<p><img src="img/BlankNetSize2.png" class="img-fluid"></p>
<p>The network parameters are:</p>
<ul>
<li>hidden layer: <span class="math inline">\(W^{1} = \begin{bmatrix} 2 &amp; -0.5 \\ -1 &amp; 1.5 \end{bmatrix}\)</span>, <span class="math inline">\(b^{1} = \begin{bmatrix} 0.25 \\ 0.25\end{bmatrix}\)</span></li>
<li>output layer: <span class="math inline">\(W^{2} = \begin{bmatrix} 0.5 &amp; -1 \end{bmatrix}\)</span>, <span class="math inline">\(b^{2} = 2\)</span></li>
</ul>
<p>Both the hidden layer and the output layer have sigmoid activation function.</p>
<ol type="a">
<li><p>Compute the network output for the input <span class="math inline">\(x = \begin{bmatrix} 0.5 \\ 1 \end{bmatrix}\)</span></p></li>
<li><p>Compute the network output for the same input, considering that the hidden layer has ReLU activation instead (output layer still sigmoid)</p></li>
<li><p>Compute the cost function (i.e.&nbsp;the loss), assuming the true output should have been <span class="math inline">\(y = 0\)</span>.</p></li>
<li><p>Compute the cost function (i.e.&nbsp;the loss), assuming the true output should have been <span class="math inline">\(y = 1\)</span>.</p></li>
<li><p>Suppose we have a network with input with size 100, one hidden layer of with 1000 neurons, and one output layer with 4 neurons.</p>
<p>How many parameters has the network? Compute the number of parameters for every layer.</p></li>
</ol>
</section>
<section id="exercise-1---wine-classification---nprtool" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="exercise-1---wine-classification---nprtool"><span class="header-section-number">3.3</span> Exercise 1 - Wine classification - nprtool</h2>
<p>We shall work with the same <code>wine_dataset</code> data as in the first labs. The last column gives the quality score. We shall treat the quality score as a <strong>class indicator</strong>. We shall perform classification with a multilayer network model, aiming to classify correctly the quality of a wine based on its parameters.</p>
<ol type="1">
<li><p>Prepare the data</p>
<p>Load the <code>wine_dataset</code> in Matlab, and consider the following:</p>
<ul>
<li>how many inputs are there?</li>
<li>how many output categories?</li>
<li>how many hidden neurons should we use?</li>
</ul>
<div class="sourceCode" id="cb5"><pre class="sourceCode matlab code-with-copy"><code class="sourceCode matlab"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="va">Data</span> <span class="op">=</span> <span class="va">readmatrix</span>(<span class="ss">'winequality-red.csv'</span>)<span class="op">;</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="va">X</span> <span class="op">=</span> <span class="va">Data</span>(<span class="op">:,</span><span class="fl">1</span><span class="op">:</span><span class="fl">11</span>)<span class="op">;</span>       <span class="co">% 11 columns for the inputs</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="va">N</span> <span class="op">=</span> <span class="va">size</span>(<span class="va">Data</span><span class="op">,</span><span class="fl">1</span>)<span class="op">;</span>       <span class="co">% The number of wines in the set (1599)</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="va">Y</span> <span class="op">=</span> <span class="va">Data</span>(<span class="op">:,</span><span class="fl">12</span>) <span class="op">;</span>        <span class="co">% 12th column the quality</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We do <strong>one hot encoding</strong> of the output vector Y. In R2020b we can use the function <code>onehotencode()</code>. In previous versions, we can use the function <code>ind2vec()</code>.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode matlab code-with-copy"><code class="sourceCode matlab"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="va">Yenc</span> <span class="op">=</span> <span class="va">full</span>(<span class="va">ind2vec</span>(<span class="va">Y</span><span class="op">'</span>))</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co">% Since our data starts from value 3, we shall remove the first two lines.</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co">% Thus quality 3 = class 1, quality 4 = class 2, etc.</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="va">Yenc</span> <span class="op">=</span> <span class="va">Yenc</span>(<span class="fl">3</span><span class="op">:</span><span class="kw">end</span><span class="op">,</span> <span class="op">:</span>)<span class="op">'</span> <span class="co">% Remove first two lines</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="va">Yenc</span> <span class="op">=</span> <span class="va">Yenc</span><span class="op">';</span>          <span class="co">% Transpose, so each value corresponds to a row, just like the input data</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Start the <code>nprtool()</code> and train a neural network to predict the wine quality.</p>
<p>Display the following evaluation metrics:</p>
<ul>
<li>Number of epochs and final error value</li>
<li>Error plot</li>
<li>Confusion matrix</li>
<li>Receiver Operating Characteristic</li>
</ul></li>
<li><p>Go back and change the hidden layer size to 20, 50, then 5. Do the results change significantly?</p></li>
</ol>
</section>
<section id="exercise-2---wine-classification---classification-learner" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="exercise-2---wine-classification---classification-learner"><span class="header-section-number">3.4</span> Exercise 2 - Wine classification - Classification Learner</h2>
<p>Use the Classification Learner app to train a similar model with two hidden layers with sizes 9 and 8.</p>
</section>
<section id="exercise-3---handwritten-digit-classification-on-mnist" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="exercise-3---handwritten-digit-classification-on-mnist"><span class="header-section-number">3.5</span> Exercise 3 - Handwritten digit classification on MNIST</h2>
<p>We follow the MNIST training tutorial from <a href="https://www.mathworks.com/matlabcentral/fileexchange/73010-mnist-neural-network-training-and-testing">https://www.mathworks.com/matlabcentral/fileexchange/73010-mnist-neural-network-training-and-testing</a></p>
<ol type="1">
<li><p>Unzip the MNIST data file `<code>MNISTdata.zip</code> provided with the lab.</p></li>
<li><p>Load the data (this will create the matrix <code>mnist_train</code>):</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode matlab code-with-copy"><code class="sourceCode matlab"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="va">load</span> <span class="va">mnist_train</span>.<span class="va">csv</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The first column is the digit number. The remaining 748 columns contain the pixels in the linearized image (every image has 28x28 pixels).</p>
<p>Lookup the MNIST database on the Internet to see how the images look like.</p></li>
<li><p>Prepare the inputs and outputs. Check the sizes of the resulting arrays to understand what they contain.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode matlab code-with-copy"><code class="sourceCode matlab"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="va">X</span> <span class="op">=</span> <span class="va">mnist_train</span>(<span class="op">:,</span><span class="fl">2</span><span class="op">:</span><span class="kw">end</span>)<span class="op">;</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="va">Y</span> <span class="op">=</span> <span class="va">mnist_train</span>(<span class="op">:,</span><span class="fl">1</span>)<span class="op">;</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co">% digits are 0 to 9, add 1 so that class indices are 1 to 10 (10 classes)</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="va">Yenc</span> <span class="op">=</span> <span class="va">full</span>(<span class="va">ind2vec</span>(<span class="fl">1</span> <span class="op">+</span> <span class="va">Y</span><span class="op">'</span>))</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co">% Transpose, so each value corresponds to a row, just like the input data</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="va">Yenc</span> <span class="op">=</span> <span class="va">Yenc</span><span class="op">';</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Use <code>patternnet()</code> to design and train a neural network with two layers of sizes 80 and 60.</p>
<p><strong>Note:</strong> Training a real-life network may take some time. Have patience until training is finished.</p>
<p>You can look a the performance and confusion matrix updating live as training progresses.</p></li>
<li><p>When training is finished:</p>
<ul>
<li>Display the error plot. Do you notice the <strong>overfitting</strong>? This is what validation is good for.</li>
<li>Display the confusion matrix. What is the classification accuracy (percentage of correctly classified images) on the testing set?</li>
</ul></li>
<li><p>Load the test file <code>mnist_test.csv</code> and predict the class of the first data sample. Is the class correct?</p></li>
<li><p>Play with parameters. Change the network architecture: use a single hidden layer, change the number of neurons, etc. Compare the results:</p>
<ul>
<li>When is the accuracy greater?</li>
<li>When is overfitting more of a problem?</li>
</ul></li>
<li><p>Optional: Use the model to predict the digit from your own hand-drawing.</p>
<p>Draw a digit similar to the ones in the dataset (this amounts to the <strong>preprocessing</strong> of the image):</p>
<ul>
<li>Draw a letter using the mouse, in MS Paint</li>
<li>Make sure the digit is around the center of the image, has the same brush width compared to the image as the ones in the dataset</li>
<li>Save it as a grayscale image</li>
<li>Resize the image to 28x28 pixels</li>
<li>You should get an image similar to the ones in the dataset</li>
</ul>
<p>Read the image in Matlab:</p>
<ul>
<li>Use <code>imread()</code> to load the image</li>
<li>Convert to double with (<code>double()</code>)</li>
<li>Linearize the matrix in a row-major order (row by row), e.g.: <code>Ivec = I'(:)</code></li>
</ul>
<p>Identify (predict) the digit using the network:</p>
<pre><code>net(Ivec)</code></pre>
<p>Was the result correct?</p></li>
</ol>
</section>
<section id="exercise-4---classifying-wheat-seeds-independent-work" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="exercise-4---classifying-wheat-seeds-independent-work"><span class="header-section-number">3.6</span> Exercise 4 - Classifying wheat seeds (independent work)</h2>
<p>Do a similar analysis on a new database, <code>seeds.csv</code>. It contains measurement data of seeds coming from three types of wheat grains.</p>
<p>Start from the template file in <code>Ex4_Template.m</code> and fill in the blanks.</p>
<ol type="1">
<li><p>Use 1 hidden layer with 50 neurons, and run the training for 1000 epochs.</p>
<ul>
<li>Does overfitting happen? (Yes / No)</li>
<li>At which epoch did the training actually stop?</li>
</ul></li>
<li><p>Normalization or not.</p>
<p>Use 1 hidden layer with 10 neurons, but run with and then without data normalization. Which case produces the best results?</p></li>
<li><p>Architecture experimentation:</p>
<p>Use a single hidden layer with different sizes: 5, 10, 15, 25, 100. Which case gives the best classification accuracy on the test set?</p></li>
<li><p>Architecture experimentation:</p>
<p>Adjust number of hidden layers (1, 2, 3, 5, 8), keeping 10 neurons in each layer. Which case gives the best classification accuracy on the test set?</p></li>
</ol>
</section>
</section>
<section id="final-questions" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Final questions</h1>
<ol type="1">
<li><p>How many parameters does the network used for MNIST classification have?</p></li>
<li><p>How many parameters would have a network used for classifying color images with 1024 x 768 resolution, into 10 output classes, using two hidden layers of size 2000 and 150?</p></li>
</ol>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>