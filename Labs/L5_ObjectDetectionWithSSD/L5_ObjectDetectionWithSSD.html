<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Object Detection networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="L5_ObjectDetectionWithSSD_files/libs/clipboard/clipboard.min.js"></script>
<script src="L5_ObjectDetectionWithSSD_files/libs/quarto-html/quarto.js"></script>
<script src="L5_ObjectDetectionWithSSD_files/libs/quarto-html/popper.min.js"></script>
<script src="L5_ObjectDetectionWithSSD_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="L5_ObjectDetectionWithSSD_files/libs/quarto-html/anchor.min.js"></script>
<link href="L5_ObjectDetectionWithSSD_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="L5_ObjectDetectionWithSSD_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="L5_ObjectDetectionWithSSD_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="L5_ObjectDetectionWithSSD_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="L5_ObjectDetectionWithSSD_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#objective" id="toc-objective" class="nav-link active" data-scroll-target="#objective"><span class="header-section-number">1</span> Objective</a></li>
  <li><a href="#theoretical-aspects" id="toc-theoretical-aspects" class="nav-link" data-scroll-target="#theoretical-aspects"><span class="header-section-number">2</span> Theoretical aspects</a>
  <ul class="collapse">
  <li><a href="#object-detection" id="toc-object-detection" class="nav-link" data-scroll-target="#object-detection">Object detection</a></li>
  <li><a href="#the-ssd-network-architecture" id="toc-the-ssd-network-architecture" class="nav-link" data-scroll-target="#the-ssd-network-architecture">The SSD network architecture</a></li>
  <li><a href="#output-encoding-and-decoding" id="toc-output-encoding-and-decoding" class="nav-link" data-scroll-target="#output-encoding-and-decoding">Output encoding and decoding</a></li>
  <li><a href="#non-max-suppression" id="toc-non-max-suppression" class="nav-link" data-scroll-target="#non-max-suppression">Non-Max Suppression</a></li>
  <li><a href="#the-model" id="toc-the-model" class="nav-link" data-scroll-target="#the-model">The model</a></li>
  <li><a href="#the-model-parameters" id="toc-the-model-parameters" class="nav-link" data-scroll-target="#the-model-parameters">The model parameters</a></li>
  <li><a href="#the-cost-function" id="toc-the-cost-function" class="nav-link" data-scroll-target="#the-cost-function">The cost function</a></li>
  <li><a href="#training" id="toc-training" class="nav-link" data-scroll-target="#training">Training</a></li>
  </ul></li>
  <li><a href="#matlab-tutorials-walkthrough" id="toc-matlab-tutorials-walkthrough" class="nav-link" data-scroll-target="#matlab-tutorials-walkthrough"><span class="header-section-number">3</span> Matlab tutorials walkthrough</a></li>
  <li><a href="#practical-work" id="toc-practical-work" class="nav-link" data-scroll-target="#practical-work"><span class="header-section-number">4</span> Practical work</a>
  <ul class="collapse">
  <li><a href="#exercise-1---run-on-another-image" id="toc-exercise-1---run-on-another-image" class="nav-link" data-scroll-target="#exercise-1---run-on-another-image">Exercise 1 - Run on another image</a></li>
  <li><a href="#exercise-2---aspect-ratio" id="toc-exercise-2---aspect-ratio" class="nav-link" data-scroll-target="#exercise-2---aspect-ratio">Exercise 2 - Aspect ratio</a></li>
  <li><a href="#exercise-3---partial-objects" id="toc-exercise-3---partial-objects" class="nav-link" data-scroll-target="#exercise-3---partial-objects">Exercise 3 - Partial objects</a></li>
  <li><a href="#exercise-4---yolox" id="toc-exercise-4---yolox" class="nav-link" data-scroll-target="#exercise-4---yolox">Exercise 4 - YoloX</a></li>
  <li><a href="#exercise-5---non-max-suppression" id="toc-exercise-5---non-max-suppression" class="nav-link" data-scroll-target="#exercise-5---non-max-suppression">Exercise 5 - Non-max suppression</a></li>
  <li><a href="#exercise-6---ssd" id="toc-exercise-6---ssd" class="nav-link" data-scroll-target="#exercise-6---ssd">Exercise 6 - SSD</a></li>
  </ul></li>
  <li><a href="#final-questions" id="toc-final-questions" class="nav-link" data-scroll-target="#final-questions"><span class="header-section-number">5</span> Final questions</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="L5_ObjectDetectionWithSSD.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li><li><a href="L5_ObjectDetectionWithSSD.ipynb" download="L5_ObjectDetectionWithSSD.ipynb"><i class="bi bi-journal-code"></i>Jupyter</a></li></ul></div></nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Object Detection networks</h1>
<p class="subtitle lead">ML Laboratory 05</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="objective" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="objective"><span class="header-section-number">1</span> Objective</h2>
<p>Students should understand the principles of object detection with single-stage object detection networks, and be able use a pretrained object detection model available in Matlab (Yolo / SSD).</p>
</section>
<section id="theoretical-aspects" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="theoretical-aspects"><span class="header-section-number">2</span> Theoretical aspects</h2>
<p>Single Shot Detector (SSD) is a CNN network architecture designed for fast detection (e.g.&nbsp;localization) of objects in an image.</p>
<section id="object-detection" class="level3">
<h3 class="anchored" data-anchor-id="object-detection">Object detection</h3>
<p>Object detection = locating certain objects in an image, and indicate their class.</p>
<p>An example<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> is provided in the image below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/DetectionExample.jpg" class="img-fluid figure-img"></p>
<figcaption>Object detection</figcaption>
</figure>
</div>
<p>The model is trained on a series of images from different categories (e.g.&nbsp;person, baseball bat, car, bicycle, , ball etc.).</p>
<p>when predicting on a new image, the model returns a list of detected objects, each object having the following data:</p>
<ol type="1">
<li><p>The <strong>bounding box</strong> surrounding the object. This is defined by 4 pixel coordinates (e.g.&nbsp;bottom=140, top=300, left=74, right=128), but can come in different varieties:</p>
<ul>
<li>(left, top, right, bottom), or in another order</li>
<li>(left, top, width, height)</li>
<li>(center_x, center_y, semi-width, semi-height)</li>
<li>etc</li>
</ul></li>
<li><p>The <strong>class</strong> of the object (i.e.&nbsp;it is a person or a bicycle). The class will come with a certain confidence score (probability), just like we saw in Lab4 for classification networks. The model estimates a score for each possible class, and the highest score wins.</p></li>
</ol>
<p>There may be some variations, depending on the model architecture used. For example, the YOLO models provide an additional “objectness” score indicating the confidence that the box is an actual object, regardless of class.</p>
</section>
<section id="the-ssd-network-architecture" class="level3">
<h3 class="anchored" data-anchor-id="the-ssd-network-architecture">The SSD network architecture</h3>
<p>The SSD (Single-Shot Detector) model architecture is presented below<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/SSDArchitecture.jpg" class="img-fluid figure-img"></p>
<figcaption>SSD architecture</figcaption>
</figure>
</div>
<p>YOLO models have a different, but similar, architecture.</p>
<p>The <strong>input</strong> of the network is a fixed-size <span class="math inline">\(W \times H \times 3\)</span> image tensor.</p>
<p>The <strong>outputs</strong> of the network are the parameters for all <span class="math inline">\(N\)</span> detection boxes:</p>
<ul>
<li>A matrix of <span class="math inline">\(N \times 4\)</span> values with the predicted box displacements/distortions with respect to all the <span class="math inline">\(N\)</span> anchor boxes (i.e.&nbsp;offset x, offset y, height scaling, width scaling).</li>
<li>A matrix of <span class="math inline">\(N \times C\)</span> with the predicted class scores for all the <span class="math inline">\(N\)</span> boxes. <span class="math inline">\(C\)</span> is the number of distinct classes in the dataset.</li>
</ul>
<p><span class="math inline">\(N\)</span> is the total number of predicted boxes over all the image. It can be very large, e.g.&nbsp;8732 boxes for a <span class="math inline">\(300 \times 300\)</span> image.</p>
<p>For a larger <span class="math inline">\(1024 \times 640\)</span> image, the number can be much larger, over 90000 (speaking from experience). Of course, it depends on the parameters chosen by the designer.</p>
</section>
<section id="output-encoding-and-decoding" class="level3">
<h3 class="anchored" data-anchor-id="output-encoding-and-decoding">Output encoding and decoding</h3>
<p>We have an image with one box around an object, done by a human (ground-truth). How do we tell the network what it should detect, in the training phase?</p>
<p><strong>Output Encoding</strong>: Before training, we must convert the ground-truth box into displacements with respect to the anchor boxes of the network. This is known as <strong>encoding</strong>: we convert the desired box in the native format of the models’ output. During training, we show the image to the network, and we “tell it” to predict the correct displacements. An example is provided below<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/SSDBox.png" class="img-fluid figure-img"></p>
<figcaption>SSD Detection Boxes</figcaption>
</figure>
</div>
<p><strong>Output Decoding</strong>: When the network runs, it outputs the predicted displacements with respect to the anchor boxes. Afterwards, from these displacements we compute the final object position (in pixels). This is known as <strong>decoding</strong> the results of the network, since we translate the native network output into our desired format (pixels). This is typically done outside the network itself.</p>
</section>
<section id="non-max-suppression" class="level3">
<h3 class="anchored" data-anchor-id="non-max-suppression">Non-Max Suppression</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="img/Non-Maximal-Suppression.png" class="img-fluid figure-img"></p>
<figcaption>Non-Max Suppression</figcaption>
</figure>
</div>
<p>This is a key post-processing step in most object detectors.</p>
<p>A single object is predicted by many anchor boxes. As a result, we will have <strong>many predicted boxes around one object</strong> in the image, with slightly different values. We want just one box. Non-Max Suppression (NMS) is a procedure to select a single box out of a groups of overlapping boxes (the one with the highest confidence score), and ignore all the other opverlapping ones. An example illustrated below<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</p>
</section>
<section id="the-model" class="level3">
<h3 class="anchored" data-anchor-id="the-model">The model</h3>
<p>The network model is defined by the sequence of convolutional layers in the architecture. Open the newtork in Matlab in order to inspect the architure in detail.</p>
</section>
<section id="the-model-parameters" class="level3">
<h3 class="anchored" data-anchor-id="the-model-parameters">The model parameters</h3>
<p>The SSD model parameters are the parameters of all the convolutional the layers. There are no fully-connected layers in the SSD network, and neither in YOLO.</p>
</section>
<section id="the-cost-function" class="level3">
<h3 class="anchored" data-anchor-id="the-cost-function">The cost function</h3>
<p>The model produces two kinds of data:</p>
<ul>
<li>box locations (expressed as displacements/distortions with respect to the anchot boxes)</li>
<li>box classification scores</li>
</ul>
<p>When training, the predicted boxes are compared to the <strong>real ground-truth</strong> boxes available. For each ground-truth box, the best predicted box is selected (the predicted box with the correct class which overlaps the most with it), and the error is computed based on two terms:</p>
<ol type="1">
<li><p>The <strong>localization error</strong>: typically, is the absolute difference between the coordinates or displacements of the ground-truth box and the predicted box.</p></li>
<li><p>The <strong>classification error</strong>, which compares the class scores of the predicted box with the true ground-truth class. The cross-entropy loss function is typically used, just like it is used for all other classification tasks.</p>
<p>Besides the predicted boxes which are compared to the ground-truth, there are many predicted boxes which corespond to <strong>no</strong> ground truth box. There are a few different methods how to treat these, some methods (e.g.&nbsp;Focal Loss) being better than others.</p></li>
</ol>
</section>
<section id="training" class="level3">
<h3 class="anchored" data-anchor-id="training">Training</h3>
<p>Training is done with <strong>backpropagation</strong> and gradient descent (or some variant of it).</p>
<p><strong>Backpropagation</strong> = the technique to compute the derivatives of <span class="math inline">\(J\)</span> with respect to all parameters in the network.</p>
<p>This is similar to all CNN networks.</p>
</section>
</section>
<section id="matlab-tutorials-walkthrough" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="matlab-tutorials-walkthrough"><span class="header-section-number">3</span> Matlab tutorials walkthrough</h2>
<p>Practical work is based on 3 Matlab examples:</p>
<p><!-- 1. [Multiclass Object Detection Using YOLO v2 Deep Learning](https://www.mathworks.com/help/vision/ug/multiclass-object-detection-using-yolo-v2-deep-learning.html) --></p>
<ol start="2" type="1">
<li><p>Using the pretrained detectors available in <a href="https://github.com/matlab-deep-learning/MATLAB-Deep-Learning-Model-Hub#object-detection-">Mathworks Github</a></p>
<ul>
<li>Use <a href="https://github.com/matlab-deep-learning/pretrained-efficientdet-d0">EfficientDet</a></li>
<li>Download the zip file of the whole repository and unzip in Matlab Online</li>
<li>Run the <code>efficientDetD0ObjectDetectionExample.m</code> file</li>
</ul></li>
<li><p>Change the thresholds in postprocess.m file</p></li>
</ol>
<p>Other tutorials:</p>
<ol type="1">
<li><a href="https://www.mathworks.com/help/vision/ug/getting-started-with-ssd.html">Getting Started with SSD Multibox Detection</a></li>
<li><a href="https://www.mathworks.com/help/vision/ug/anchor-boxes-for-object-detection.html">Anchor Boxes for Object Detection</a></li>
<li><a href="https://www.mathworks.com/help/vision/ug/object-detection-using-single-shot-detector.html">Object Detection Using SSD Deep Learning</a></li>
<li><a href="https://www.mathworks.com/help/vision/ug/object-detection-using-yolov4-deep-learning.html">Object Detection Using YOLO v4 Deep Learning</a></li>
</ol>
</section>
<section id="practical-work" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="practical-work"><span class="header-section-number">4</span> Practical work</h2>
<section id="exercise-1---run-on-another-image" class="level3">
<h3 class="anchored" data-anchor-id="exercise-1---run-on-another-image">Exercise 1 - Run on another image</h3>
<ul>
<li>Run the EffecientDet object detector on the image <code>Richarlison.jpg</code>, and identify how many objects are detected, and their class.</li>
<li>Set the overlap threshold to 0.01 and run it on the <code>Richarlison.jpg</code> image. What happens?</li>
<li>Set the overlap threshold to 0.9 and the score threshold to 0.2, and run it on the <code>Richarlison.jpg</code> image. What happens?</li>
<li>Set a breakpoint on line 34 of the <code>postprocess.m</code> file and investigate the size of the <code>scores</code> vector. What is its size? What does this number represent?</li>
</ul>
</section>
<section id="exercise-2---aspect-ratio" class="level3">
<h3 class="anchored" data-anchor-id="exercise-2---aspect-ratio">Exercise 2 - Aspect ratio</h3>
<ul>
<li><p>Run the EfficientDet object detector on the image <code>CarWide.jpg</code>. Is the car detected? If yes, with what score?</p></li>
<li><p>Identify the input resolution of the EfficientDet network.</p></li>
<li><p>Use the Matlab function <code>imresize()</code> to resize the image <code>CarWide.jpg</code> to the input resolution, and display it with <code>imshow()</code>.</p>
<p>What happens with the car image? This is the image which goes into the object detector network. Do you thinks this a problem?</p></li>
<li><p>Use any Windows tool to crop from the <code>CarWide.jpg</code> a square part containing the car, save it, and run the object detector on it.</p>
<p>Does the detection score improve?</p></li>
</ul>
</section>
<section id="exercise-3---partial-objects" class="level3">
<h3 class="anchored" data-anchor-id="exercise-3---partial-objects">Exercise 3 - Partial objects</h3>
<ul>
<li><p>Use any Windows tool to crop from the <code>CarWide.jpg</code> a square part containing</p>
<ul>
<li>about 3/4 of the car</li>
<li>about 1/2 of the car</li>
<li>about 1/4 of the car</li>
</ul></li>
<li><p>Save each image and run the object detector on each of them. What is the detection score?</p></li>
</ul>
</section>
<section id="exercise-4---yolox" class="level3">
<h3 class="anchored" data-anchor-id="exercise-4---yolox">Exercise 4 - YoloX</h3>
<ul>
<li>Go to the YoloX object detector from here: https://github.com/matlab-deep-learning/Pretrained-YOLOX-Network-For-Object-Detection</li>
<li>Using the Github example, run this detector on the same image <code>Richarlison.jpg</code></li>
<li>Filter the bounding box on the image:
<ul>
<li>Display only the objects with the highest score</li>
<li>Display only the object whose center (the center of the box) is the left-most among all objects</li>
<li>Display only the non-person objects</li>
</ul></li>
<li>Identify the two threshold values in the <code>postprocess.m</code> function, the score threshold and the overlap threshold.</li>
<li>Set both thresholds to 0.03 and display the results. How many objects are detected now?</li>
<li>Set the score threshold to 0.1 and the overlap threshold to 0.8. How many objects are detected now?</li>
<li>What is the total number of boxes predicted by the YoloX model?</li>
</ul>
</section>
<section id="exercise-5---non-max-suppression" class="level3">
<h3 class="anchored" data-anchor-id="exercise-5---non-max-suppression">Exercise 5 - Non-max suppression</h3>
<p>Write a small Matlab function called <code>IOU</code> which computes the intersection-over-union of two bounding boxes, given as vectors <code>[left, right, bottom, top]</code>.</p>
<p>Use the <code>rectint()</code> function to compute the intersection of the boxes.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode matlab code-with-copy"><code class="sourceCode matlab"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="va">result</span> <span class="op">=</span> <span class="va">IOU</span>(<span class="va">box1</span><span class="op">,</span> <span class="va">box2</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">% Compute IOU of two boxes</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">% Each box is a 4-element vector: [left, right, bottom, top]</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="op">...</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Use the function to compute the IOU of the following two bounding boxes:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode matlab code-with-copy"><code class="sourceCode matlab"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="va">A</span> <span class="op">=</span> [<span class="fl">50</span><span class="op">,</span> <span class="fl">100</span><span class="op">,</span> <span class="fl">120</span><span class="op">,</span> <span class="fl">200</span>]<span class="op">;</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="va">B</span> <span class="op">=</span> [<span class="fl">10</span><span class="op">,</span> <span class="fl">80</span><span class="op">,</span> <span class="fl">40</span><span class="op">,</span> <span class="fl">140</span>]<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="exercise-6---ssd" class="level3">
<h3 class="anchored" data-anchor-id="exercise-6---ssd">Exercise 6 - SSD</h3>
<ol type="1">
<li><p>Run the first steps of the <a href="https://www.mathworks.com/help/vision/ug/object-detection-using-single-shot-detector.html">Object Detection Using SSD Deep Learning</a> tutorial, and stop before the creation of the SSD network.</p></li>
<li><p>Investigate the network architecture by running <code>analyzeNetwork()</code> on the network variable <code>lgraph</code></p>
<ul>
<li>How many layers are there?</li>
<li>What is the required size of the input image?</li>
<li>Which are the two output layers?</li>
<li>How many boxes are predicted in all?</li>
</ul></li>
<li><p>Run the next steps of the tutorial, and observer the detected boxes on the image.</p></li>
<li><p>Download a similar image from the Internet and run the model on it. Are the objects detected well?</p></li>
<li><p>Locate the call to the <code>detect()</code> function and change the detection threshold to 0.01. What happens?</p>
<ul>
<li>What happens if the threshold is set too low?</li>
<li>What would happen if the threshold is set too high?</li>
<li>What is the trade-off involved in choosing a value for the threshold?</li>
</ul></li>
<li><p>Set the detection parameter ‘SelectStrongest’ to <code>false</code>: <code>detect(... 'SelectStrongest', false)</code>. Set the threshold to 0.001. What changes? How many detected boxes are now?</p></li>
<li><p>Set the detection parameter ‘SelectStrongest’ to <code>false</code>: <code>detect(... 'SelectStrongest', false)</code>. Reset the threshold to a reasonable value (e.g.&nbsp;0.4). What changes? How many detected boxes are now?</p>
<ul>
<li>Why do we have multiple boxes around a single object?</li>
<li>Imagine a procedure to keep only a single detection box around an object (this is known as <strong>Non-Max Suppresion (NMS)</strong>)</li>
</ul></li>
<li><p>Set the variable <code>doTraining</code> to <code>true</code> (or 1) and train the model. How fast does it work?</p></li>
</ol>
</section>
</section>
<section id="final-questions" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="final-questions"><span class="header-section-number">5</span> Final questions</h2>
<p>TBD</p>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>image from <a href="https://lambdalabs.com/blog/how-to-implement-ssd-object-detection-in-tensorflow/">https://lambdalabs.com/blog/how-to-implement-ssd-object-detection-in-tensorflow/</a><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>image from <a href="https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab">https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>image from the SSD paper <a href="https://arxiv.org/abs/1512.02325">https://arxiv.org/abs/1512.02325</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>image from <a href="https://www.researchgate.net/publication/345061606_Incremental_Training_for_Image_Classification_of_Unseen_Objects">Incremental Training for Image Classification of Unseen Objects</a><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>